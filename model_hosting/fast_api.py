from fastapi import File, UploadFile, Form, APIRouter, Request, HTTPException
from fastapi.responses import JSONResponse
from typing import List, Optional, Literal, Any
import tempfile
import os
import re
import pymysql
from duckduckgo_search import DDGS  
from pydantic import BaseModel
from extraction.file_base_extraction import get_extractor_by_extension
from extraction.pdf_extraction import PDFExtraction
from extraction.prompt_extraciont import PromptExtraction
from ollama_load.ollama_hosting import OllamaHosting
import requests
import httpx
from dotenv import load_dotenv
import torch
import whisperx
import ollama
import json
from langchain_core.messages import get_buffer_string, AIMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableConfig
from langchain_ollama import ChatOllama
from langgraph.graph import END, START, StateGraph
from langgraph.prebuilt import ToolNode
from transformers import AutoTokenizer
from langchain_huggingface.embeddings import HuggingFaceEmbeddings
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_core.documents import Document
from langchain_core.tools import tool, StructuredTool
import uuid
from datetime import datetime
from types import SimpleNamespace
from pydub import AudioSegment

load_dotenv()
secret = os.getenv("SESSION_SECRET")

DB_HOST = os.environ.get("MY_DB_HOST", "localhost")
DB_PORT = int(os.environ.get("MY_DB_PORT", "3306"))
DB_USER = os.environ.get("MY_DB_USER", "wlb_mate")
DB_PASSWORD = os.environ.get("MY_DB_PASSWORD", "wlb_mate")  # ì‹¤ì œ ë¹„ë°€ë²ˆí˜¸ë¡œ êµì²´
DB_NAME = os.environ.get("MY_DB_NAME", "wlb_mate")
DB_CHARSET = os.environ.get("MY_DB_CHARSET", "utf8mb4")

class State(BaseModel):
    messages: List[Any] = []
    recall_memories: List[str] = []

class EmbeddingManager:
    def __init__(self, model_name: str = "BM-K/KoSimCSE-roberta"):
        self.embeddings = HuggingFaceEmbeddings(
            model_name=model_name,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        self.vector_store = InMemoryVectorStore(
            embedding=self.embeddings
        )
    def get_embeddings(self):
        return self.embeddings
    def get_vector_store(self):
        return self.vector_store

class MemoryTools:
    def __init__(self, vector_store):
        self.vector_store = vector_store

        # 1. docstring ë°©ì‹
        self.save_recall_memory_tool = tool(self._save_recall_memory_with_doc)
        self.search_recall_memories_tool = tool(self._search_recall_memories_with_doc)

        # 2. description ì§ì ‘ ì§€ì • ë°©ì‹
        self.save_recall_memory_tool2 = StructuredTool.from_function(
            func=self._save_recall_memory_no_doc,
            name="save_recall_memory",
            description="Save user's long-term memory to vector storage."
        )
        self.search_recall_memories_tool2 = StructuredTool.from_function(
            func=self._search_recall_memories_no_doc,
            name="search_recall_memories",
            description="Search memories by semantic similarity."
        )

        self.tools = [self.save_recall_memory_tool, self.search_recall_memories_tool]
        # self.tools = [self.save_recall_memory_tool2, self.search_recall_memories_tool2]

    def get_user_id(self, config: RunnableConfig) -> str:
        user_id = config["configurable"].get("user_id")
        if user_id is None:
            raise ValueError("User ID needs to be provided to save a memory.")
        return user_id

    # 1. docstring ë°©ì‹ (ì˜ì–´, Google-style)
    def _save_recall_memory_with_doc(self, memory: str, config: RunnableConfig) -> str:
        """
        Save user's long-term memory to the vector store.

        Args:
            memory (str): The memory text to save.
            config (RunnableConfig): Configuration containing user_id.

        Returns:
            str: The saved memory text.
        """
        user_id = self.get_user_id(config)
        document = Document(
            page_content=memory,
            id=str(uuid.uuid4()),
            metadata={"user_id": user_id}
        )
        self.vector_store.add_documents([document])
        return memory

    def _search_recall_memories_with_doc(self, query: str, config: RunnableConfig) -> list:
        """
        Search stored memories by semantic similarity.

        Args:
            query (str): The search query.
            config (RunnableConfig): Configuration containing user_id.

        Returns:
            list: List of matched memory texts.
        """
        user_id = self.get_user_id(config)
        def _filter_function(doc: Document) -> bool:
            return doc.metadata.get("user_id") == user_id
        documents = self.vector_store.similarity_search(
            query, k=3, filter=_filter_function
        )
        return [document.page_content for document in documents]

    def _save_recall_memory_no_doc(self, memory: str, config: RunnableConfig) -> str:
        user_id = self.get_user_id(config)
        document = Document(
            page_content=memory,
            id=str(uuid.uuid4()),
            metadata={"user_id": user_id}
        )
        self.vector_store.add_documents([document])
        return memory

    def _search_recall_memories_no_doc(self, query: str, config: RunnableConfig) -> list:
        user_id = self.get_user_id(config)
        def _filter_function(doc: Document) -> bool:
            return doc.metadata.get("user_id") == user_id
        documents = self.vector_store.similarity_search(
            query, k=3, filter=_filter_function
        )
        return [document.page_content for document in documents]

    def get_tools(self):
        return self.tools

class MySQLCheckpoint:
    """
    PyMySQL ì»¤ë„¥ì…˜ì„ ëª…ì‹œì ìœ¼ë¡œ ì—´ê³  ë‹«ëŠ” ì»¤ìŠ¤í…€ ì²´í¬í¬ì¸í„° ì˜ˆì‹œ.
    ê° ë©”ì„œë“œ í˜¸ì¶œ ì‹œ ìƒˆë¡œìš´ ì»¤ë„¥ì…˜ì„ ìƒì„±í•˜ê³  ë‹«ìŠµë‹ˆë‹¤.
    """
    def __init__(self, host, port, user, password, db, charset="utf8"):
        self.db_config = {
            "host": host,
            "port": port,
            "user": user,
            "password": password,
            "db": db,
            "charset": charset
        }
    
    def _get_connection(self):
        return pymysql.connect(**self.db_config)

    def is_chat_owner(self, chat_no: int, emp_code: str) -> bool:
        conn = self._get_connection()
        try:
            with conn.cursor() as cursor:
                cursor.execute("""
                    SELECT 1
                    FROM chat_mate m
                    JOIN employee e ON m.emp_no = e.emp_no
                    WHERE m.chat_no = %s AND e.emp_code = %s
                """, (chat_no, emp_code))
                return cursor.fetchone() is not None
        finally:
            conn.close()


    def get_tuple(self, config):
        if config["configurable"].get("new_chat"):
            return {"messages": [], "recall_memories": []}

        emp_code = config["configurable"]["user_id"]
        chat_no = config["configurable"].get("chat_no")

        conn = self._get_connection()

        if not chat_no:
            return {"messages": [], "recall_memories": []}
        
        try:
            with conn.cursor() as cursor:
                # chat_no ìœ íš¨ì„± í™•ì¸
                if not self.is_chat_owner(chat_no, emp_code):
                    raise ValueError("ê¶Œí•œì´ ì—†ëŠ” ì±„íŒ…ë°©ì…ë‹ˆë‹¤.")

                # ë©”ì‹œì§€ ì¡°íšŒ
                cursor.execute("""
                    SELECT log_text, log_speaker_sn
                    FROM chat_log
                    WHERE chat_no = %s
                    ORDER BY log_create_dt
                """, (chat_no,))
                rows = cursor.fetchall()

                messages = [
                    HumanMessage(content=log) if speaker == 1 else AIMessage(content=log)
                    for log, speaker in rows
                ]
                return {"messages": messages, "recall_memories": []}
        finally:
            conn.close()


    def save_tuple(self, config, messages: List[Any], recall_memories: List[str]):
        emp_code = config["configurable"]["user_id"]
        force_new_chat = config["configurable"].get("new_chat", False)
        chat_no = config["configurable"].get("chat_no")

        conn = self._get_connection()
        try:
            with conn.cursor() as cursor:
                # chat_no ìœ íš¨ì„± ê²€ì¦
                if chat_no and not self.is_chat_owner(chat_no, emp_code):
                    raise ValueError("ê¶Œí•œì´ ì—†ëŠ” ì±„íŒ…ë°©ì…ë‹ˆë‹¤.")

                # chat_noê°€ ì—†ìœ¼ë©´ ê°€ì¥ ìµœê·¼ ê²ƒì„ ì°¾ê±°ë‚˜ ìƒˆë¡œ ìƒì„±
                if not chat_no:
                    first_two = messages[:2]
                    user_msg = first_two[0].content if len(first_two) > 0 and isinstance(first_two[0], HumanMessage) else ""
                    bot_msg = first_two[1].content if len(first_two) > 1 and isinstance(first_two[1], AIMessage) else ""
                    title_prompt = prompt_extraction.make_chat_title_prompt(user_msg, bot_msg)
                    try:
                        title = OllamaHosting("qwen2.5", title_prompt).get_model_response().strip()
                        print("=> ìƒì„±ëœ ì œëª©:", title)
                    except Exception as e:
                        print("=> ì œëª© ìƒì„± ì‹¤íŒ¨:", e)
                        title = f"ëŒ€í™” {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
                    cursor.execute("""
                    INSERT INTO chat_mate (emp_no, chat_title)
                    SELECT emp_no, %s FROM employee WHERE emp_code = %s
                    """, (title, emp_code))
                    chat_no = cursor.lastrowid

                # ì €ì¥ëœ ë©”ì‹œì§€ ì´í›„ë§Œ ì €ì¥
                cursor.execute("SELECT COUNT(*) FROM chat_log WHERE chat_no = %s", (chat_no,))
                saved_count = cursor.fetchone()[0]
                new_messages = messages[saved_count:]

                for msg in new_messages:
                    if isinstance(msg, HumanMessage):
                        speaker_sn = 1
                    elif isinstance(msg, AIMessage):
                        speaker_sn = 2
                    elif isinstance(msg, dict):
                        if msg.get("sender") == "user":
                            speaker_sn = 1
                        elif msg.get("sender") == "bot":
                            speaker_sn = 2
                        else:
                            continue
                        msg = SimpleNamespace(content=msg.get("text", ""))
                    else:
                        continue

                    cursor.execute("""
                        INSERT INTO chat_log (chat_no, log_text, log_speaker_sn, log_create_dt)
                        VALUES (%s, %s, %s, NOW())
                    """, (chat_no, msg.content, speaker_sn))
            conn.commit()
            return chat_no
        finally:
            conn.close()



    def get_chat_list(self, emp_code):
        conn = self._get_connection()
        try:
            with conn.cursor() as cursor:
                cursor.execute("""
                    SELECT chat_no, chat_title, DATE_FORMAT(chat_create_dt, '%%Y.%%m.%%d') as chat_create_dt
                    FROM chat_mate
                    WHERE emp_no = (SELECT emp_no FROM employee WHERE emp_code = %s)
                    ORDER BY chat_create_dt DESC
                """, (emp_code,))
                rows = cursor.fetchall()
                return [
                    {
                        "chat_no": row[0],
                        "chat_title": row[1],
                        "chat_create_dt": row[2]
                    } for row in rows
                ]
        finally:
            conn.close()

    def get_chat_log(self, chat_no):
        conn = self._get_connection()
        try:
            with conn.cursor() as cursor:
                cursor.execute("""
                    SELECT log_text, log_speaker_sn
                    FROM chat_log
                    WHERE chat_no = %s
                    ORDER BY log_create_dt
                """, (chat_no,))
                rows = cursor.fetchall()

                return [
                    {"text": row[0], "sender": "user" if row[1] == 1 else "bot"}
                    for row in rows
                ]
        finally:
            conn.close()
   


class MemoryAgent:
    def __init__(self, model_name: str = "qwen2.5", tokenizer_name: str = "BM-K/KoSimCSE-roberta"):
        self.model = ChatOllama(model=model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        self.prompt = self._create_prompt()
        self.tools = None 
        self.model_with_tools = None
    def _create_prompt(self) -> ChatPromptTemplate:
        return ChatPromptTemplate.from_messages([
            (
                "system",
                "ë‹¹ì‹ ì€ ê³ ê¸‰ ì¥ê¸° ê¸°ì–µë ¥ì„ ê°€ì§„ ìœ ìš©í•œ ì¡°ìˆ˜ì…ë‹ˆë‹¤."
                " ê¸°ëŠ¥. í•œêµ­ì–´ë¥¼ ì´í•´í•˜ëŠ” LLMìœ¼ë¡œ êµ¬ë™ë˜ë¯€ë¡œ ë‹µë³€ì€ í•œêµ­ì–´ë¡œ í•´ì•¼ í•©ë‹ˆë‹¤.."
                " ëŒ€í™” ì‚¬ì´ì˜ ì •ë³´ë¥¼ ì €ì¥í•˜ëŠ” ì™¸ì¥ ë©”ëª¨ë¦¬ê°€ ìˆìŠµë‹ˆë‹¤."
                " ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì €ì¥í•˜ê³  ê²€ìƒ‰í•˜ì„¸ìš”"
                " ì‚¬ìš©ìì˜ ì£¼ì˜ë¥¼ ê¸°ìš¸ì´ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” ì¤‘ìš”í•œ ì„¸ë¶€ ì‚¬í•­"
                "ë©”ëª¨ë¦¬ ì‚¬ìš© ì§€ì¹¨:\n"
                "1. ë©”ëª¨ë¦¬ ë„êµ¬(save_recall_memory)ë¥¼ ì ê·¹ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì„¸ìš”."
                " ì‚¬ìš©ìì— ëŒ€í•œ í¬ê´„ì ì¸ ì´í•´ë¥¼ êµ¬ì¶•í•˜ê¸° ìœ„í•´.\n"
                "2. ì €ì¥ëœ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •ë³´ì— ì…ê°í•œ ê°€ì •ê³¼ ì¶”ì •ì„ í•©ë‹ˆë‹¤."
                " ì¶”ì–µ.\n"
                "3. íŒ¨í„´ì„ ì‹ë³„í•˜ê¸° ìœ„í•´ ê³¼ê±° ìƒí˜¸ì‘ìš©ì„ ì •ê¸°ì ìœ¼ë¡œ ë°˜ì„±í•˜ê³ "
                " ê¸°ë³¸ ì„¤ì •.\n"
                "4. ìƒˆë¡œìš´ ì‘í’ˆë§ˆë‹¤ ì‚¬ìš©ìì˜ ì •ì‹  ëª¨ë¸ì„ ì—…ë°ì´íŠ¸í•˜ì„¸ìš”."
                " ì •ë³´.\n"
                "5. ìƒˆë¡œìš´ ì •ë³´ì™€ ê¸°ì¡´ ê¸°ì–µì„ ìƒí˜¸ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤."
                " ì¼ê´€ì„±.\n"
                "6. ê°ì •ì  ë§¥ë½ê³¼ ê°œì¸ì  ê°€ì¹˜ë¥¼ ì €ì¥í•˜ëŠ” ê²ƒì„ ìš°ì„ ì‹œí•©ë‹ˆë‹¤."
                " ì‚¬ì‹¤ê³¼ í•¨ê»˜.\n"
                "7. ê¸°ì–µì„ ì‚¬ìš©í•˜ì—¬ í•„ìš”ë¥¼ ì˜ˆì¸¡í•˜ê³  ì´ì— ëŒ€í•œ ì‘ë‹µì„ ë§ì¶¤í™”í•˜ì„¸ìš”."
                " ì‚¬ìš©ì ìŠ¤íƒ€ì¼.\n"
                "8. ì‚¬ìš©ìì˜ ìƒí™© ë³€í™”ë¥¼ ì¸ì‹í•˜ê³  ì¸ì •í•˜ê¸° ë˜ëŠ”"
                " ì‹œê°„ì— ë”°ë¥¸ ê´€ì .\n"
                "9. ê¸°ì–µì„ í™œìš©í•˜ì—¬ ê°œì¸í™”ëœ ì˜ˆì‹œë¥¼ ì œê³µí•©ë‹ˆë‹¤."
                " ìœ ì¶”.\n"
                "10. ê³¼ê±°ì˜ ë„ì „ì´ë‚˜ ì„±ê³µì„ íšŒìƒí•˜ì—¬ í˜„ì¬ë¥¼ ì•Œë¦½ë‹ˆë‹¤."
                " ë¬¸ì œ í•´ê²°.\n\n"
                "## ì¶”ì–µ íšŒìƒ\n"
                "ì†Œí™˜ ê¸°ì–µì€ í˜„ì¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë§¥ë½ì ìœ¼ë¡œ ê²€ìƒ‰ë©ë‹ˆë‹¤."
                " ëŒ€í™”:\n{recall_memories}\n\n"
                "## ì§€ì¹¨\n"
                "ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë™ë£Œë‚˜ ì¹œêµ¬ë¡œì„œ ìì—°ìŠ¤ëŸ½ê²Œ ì‚¬ìš©ìì™€ ì†Œí†µí•˜ì„¸ìš”."
                " ë‹¹ì‹ ì˜ ê¸°ì–µ ëŠ¥ë ¥ì„ ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰í•  í•„ìš”ëŠ” ì—†ìŠµë‹ˆë‹¤."
                " ëŒ€ì‹  ì‚¬ìš©ìì— ëŒ€í•œ ì´í•´ë¥¼ ì›í™œí•˜ê²Œ í†µí•©í•˜ì„¸ìš”."
                " ë‹¹ì‹ ì˜ ë°˜ì‘ì— ê·€ë¥¼ ê¸°ìš¸ì´ì„¸ìš”. ë¯¸ë¬˜í•œ ì‹ í˜¸ì™€ ê·¼ë³¸ì ì¸ ì •ë³´ì— ì£¼ì˜í•˜ì„¸ìš”."
                " ê°ì •. ì‚¬ìš©ìì˜ ì˜ì‚¬ì†Œí†µ ìŠ¤íƒ€ì¼ì— ë§ê²Œ ì¡°ì •í•˜ì„¸ìš”."
                " ì„ í˜¸ë„ì™€ í˜„ì¬ ê°ì • ìƒíƒœ. ì§€ì†í•˜ë ¤ë©´ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
                " ë‹¤ìŒ ëŒ€í™”ì—ì„œ ìœ ì§€í•˜ê³  ì‹¶ì€ ì •ë³´. ë§Œì•½ ë‹¹ì‹ ì´"
                " ë„êµ¬ í˜¸ì¶œì„ í•˜ì„¸ìš”. ë„êµ¬ í˜¸ì¶œ ì•ì— ìˆëŠ” ëª¨ë“  í…ìŠ¤íŠ¸ëŠ” ë‚´ë¶€ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤"
                " ë©”ì‹œì§€. ë„êµ¬ë¥¼ í˜¸ì¶œí•œ í›„ ì‘ë‹µí•˜ì„¸ìš”"
                " ë„êµ¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n\n",
            ),
            ("placeholder", "{messages}"),
        ])
    def set_prompt(self, system_template: str):
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", system_template),
            ("placeholder", "{messages}")
        ])
    def setup_model_with_tools(self, tools):
        self.tools = tools
        self.model_with_tools = self.model.bind_tools(tools)
        return self.model_with_tools
    def agent(self, state: State) -> State:
        bound = self.prompt | self.model_with_tools
        recall_str = (
            "<recall_memory>\n" + "\n".join(state.recall_memories) + "\n</recall_memory>"
        )
        prediction = bound.invoke(
            {
                "messages": state.messages,
                "recall_memories": recall_str,
            }
        )
        return State(messages=[prediction], recall_memories=state.recall_memories)
    def load_memories(self, state: State, config: RunnableConfig) -> State:
        if not self.tools:
            raise ValueError("Tools have not been set up. Call setup_model_with_tools first.")
        convo_str = get_buffer_string(state.messages)
        tokens = self.tokenizer(convo_str, truncation=True, max_length=2048)
        truncated_text = self.tokenizer.decode(tokens['input_ids'])
        recall_memories = self.tools[1].invoke(truncated_text, config=config)
        return State(messages=state.messages, recall_memories=recall_memories)
    def route_tools(self, state: State) -> Literal["tools", END]:
        msg = state.messages[-1]
        if getattr(msg, "tool_calls", None):
            return "tools"
        return END

router = APIRouter()
prompt_extraction = PromptExtraction()

def search_web_duckduckgo(query: str):
    with DDGS() as ddgs:
        results = ddgs.text(query, region='kr-kr', max_results=3)
        return list(results)

def summarize_body(body: str, max_len=150) -> str:
    body = body.strip().replace("\n", " ")
    if len(body) > max_len:
        return body[:max_len].rstrip() + "..."
    return body

def clean_korean_only(text: str) -> str:
    return re.sub(r"[^ê°€-í£a-zA-Z0-9\s.,!?~\-]", "", text)

def classify_question_mode(question: str) -> str:
    keywords = ["ìœ ì‚¬ ì‚¬ì—…", "ì¸í„°ë„·ì—ì„œ ì°¾ì•„", "ì›¹ ê²€ìƒ‰", "ê²€ìƒ‰","ê²€ìƒ‰í•´ì¤˜"]
    if any(k in question.lower() for k in keywords):
        return "web_search"
    return "document"

embedding_manager = EmbeddingManager()
memory_tools = MemoryTools(embedding_manager.get_vector_store())
memory_agent = MemoryAgent()
memory_agent.setup_model_with_tools(memory_tools.get_tools())

def get_from_state(state, key, default):
    if hasattr(state, key):
        return getattr(state, key, default)
    elif isinstance(state, dict) and key in state:
        return state[key]
    elif hasattr(state, "values") and key in state.values:
        return state.values.get(key, default)
    return default

@router.post("/ask")
async def ask(
    request: Request,
    question: str = Form(...),
    files: List[UploadFile] = File(None)
):
    
    # ë””ë²„ê¹…ìš© ì„¸ì…˜ ë¡œê·¸ ì¶œë ¥
    print("ğŸ” request.session =", request.session)
    print("ğŸ” session.get('employee') =", request.session.get("employee"))
    print("ğŸ” request.cookies =", request.cookies)

    form = await request.form()
    new_chat_flag = form.get("new_chat", "false").lower() == "true"

    print("=>new_chat =", form.get("new_chat"))
    print("=>chat_no =", form.get("chat_no"))

    employee = request.session.get("employee")
    if not employee or "emp_code" not in employee:
        print("âŒ ì„¸ì…˜ ì¸ì¦ ì‹¤íŒ¨ - 401 ë°˜í™˜")
        raise HTTPException(status_code=401, detail="ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")

    user_id = employee["emp_code"]

    chat_no_str = form.get("chat_no", "").strip()
    chat_no = int(chat_no_str) if chat_no_str.isdigit() else None

    mode = classify_question_mode(question)
    config = {"configurable": {"user_id": user_id, "thread_id": user_id, "new_chat": new_chat_flag, "chat_no": chat_no}}

    # MySQLCheckpoint ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìš”ì²­ë§ˆë‹¤ ìƒì„±í•˜ì—¬ ì—°ê²°ì„ ëª…ì‹œì ìœ¼ë¡œ ê´€ë¦¬
    checkpoint = MySQLCheckpoint(
        host=DB_HOST,
        port=DB_PORT,
        user=DB_USER,
        password=DB_PASSWORD,
        db=DB_NAME,
        charset=DB_CHARSET
    )
    
    current_state_from_checkpoint = checkpoint.get_tuple(config)
    current_messages = current_state_from_checkpoint.get("messages", [])
    current_recall_memories = current_state_from_checkpoint.get("recall_memories", [])

    # Ensure current_messages is a list and append the new HumanMessage
    current_messages = list(current_messages)
    current_messages.append(HumanMessage(content=question))

    if files:
        document_texts = []
        filenames = []

        for file in files[:5]:  # ìµœëŒ€ 5ê°œ ì²˜ë¦¬
            suffix = os.path.splitext(file.filename)[1]
            with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
                tmp.write(await file.read())
                tmp_path = tmp.name

            tmp.close()

            extractor = get_extractor_by_extension(file.filename, tmp_path)
            pages = extractor.extract_text()
            os.remove(tmp_path)

            text = "\n\n".join([p['text'] for p in pages])
            document_texts.append((file.filename, text))
            filenames.append(file.filename)

            page_texts = [p['text'] for p in pages]
            async with httpx.AsyncClient(timeout=300.0) as client:
                await client.post(
                    "http://localhost:8002/api/upload_vectors",
                    json={"chunks": page_texts, "collection_name": "qdrant_temp"}
                )


        async with httpx.AsyncClient(timeout=300.0) as client:
            search_resp = await client.post(
                "http://localhost:8002/api/search_vectors",
                json={"question": question, "collection_name": "qdrant_temp"}
            )

        search_data = search_resp.json()
        context_texts = search_data.get("result", "")
        context = "\n".join(context_texts if isinstance(context_texts, list) else [context_texts])


        # ì›¹ ê²€ìƒ‰ ëª¨ë“œ: ì²« ë²ˆì§¸ íŒŒì¼ ê¸°ì¤€ìœ¼ë¡œ ê²€ìƒ‰ì–´ ì¶”ì¶œ
        if mode == "web_search":
            first_text = document_texts[0][1]
            keyword_prompt = prompt_extraction.make_keyword_extraction_prompt(first_text)
            ollama_extract_keywords = OllamaHosting("qwen2.5", keyword_prompt)
            search_query = ollama_extract_keywords.get_model_response().strip()
            results = search_web_duckduckgo(search_query)

            results_text = "\n\n".join(
                [
                    f"ì œëª©: {res.get('title', '(ì œëª©ì—†ìŒ)')}\n"
                    f"ë‚´ìš©: {summarize_body(res.get('body', ''))}\n"
                    f"ì‚¬ì´íŠ¸ ì£¼ì†Œ: {res.get('href', '(ì£¼ì†Œì—†ìŒ)')}"
                    for res in results
                ]
            )
            # ì›¹ ê²€ìƒ‰ ê²°ê³¼ëŠ” DBì— ì €ì¥í•˜ì§€ ì•ŠëŠ” ê²ƒìœ¼ë¡œ íŒë‹¨í•˜ì—¬, ì´ì „ ë©”ì‹œì§€ë§Œ ì €ì¥
            chat_no = checkpoint.save_tuple(config, current_messages, current_recall_memories)
            return {
                "answer": f"ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìœ ì‚¬ ì‚¬ì—…ì„ ê²€ìƒ‰í•œ ê²°ê³¼ì…ë‹ˆë‹¤ (ê²€ìƒ‰ì–´: {search_query}):\n\n{results_text}",
                "evaluation_criteria": "í•´ë‹¹ ëª¨ë“œì—ì„œëŠ” í‰ê°€ ê¸°ì¤€ ì¶”ì¶œì´ ì œê³µë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
                "chat_no": chat_no
            }

        # ì¼ë°˜ ë¬¸ì„œ ì§ˆì˜ ì‘ë‹µ ëª¨ë“œ
        answers = []
        evaluation_criteria = None

        for filename, text in document_texts:
            # í•œ ë²ˆë§Œ ì¶”ì¶œ
            if evaluation_criteria is None:
                async with httpx.AsyncClient(timeout=300.0) as client:
                    criteria_resp = await client.post(
                        "http://localhost:8002/api/search_vectors",
                        json={"question": "í‰ê°€ ê¸°ì¤€", "collection_name": "qdrant_temp"}
                    )
                criteria_data = criteria_resp.json()
                criteria_list = criteria_data.get("result", "")
                evaluation_criteria = "\n".join(criteria_list if isinstance(criteria_list, list) else [criteria_list])

            agent_state = State(messages=current_messages, recall_memories=current_recall_memories)
            agent_state = memory_agent.load_memories(agent_state, config)
            recall_memories_text = "\n".join(agent_state.recall_memories)

            qa_prompt = prompt_extraction.make_prompt_to_query_document(context, question, recall_memories_text)
            memory_agent.set_prompt(qa_prompt)

            agent_response = memory_agent.agent(agent_state)
            answer = get_from_state(agent_response, "messages", [])[-1].content.strip()

            answers.append(f"{answer}\n\n**ì¶œì²˜: {filename}**")

        agent_response_content = "\n\n---\n\n".join(answers)
        ai_message = AIMessage(content=agent_response_content)
        all_messages = current_messages + [ai_message]
        chat_no = checkpoint.save_tuple(config, all_messages, current_recall_memories)
        return {
            "answer": agent_response_content,
            "evaluation_criteria": evaluation_criteria,
            "chat_no": chat_no
        }

    # ë¬¸ì„œ ì—†ìŒ + ì›¹ ê²€ìƒ‰
    elif mode == "web_search":
        search_query = question.strip()
        results = search_web_duckduckgo(search_query)
        results_text = "\n\n".join(
            [
                f"ì œëª©: {res.get('title','(ì œëª©ì—†ìŒ)')}\n"
                f"ë‚´ìš©: {summarize_body(res.get('body',''))}\n"
                f"ì‚¬ì´íŠ¸ ì£¼ì†Œ: {res.get('href','(ì£¼ì†Œì—†ìŒ)')}"
                for res in results
            ]
        )
        chat_no = checkpoint.save_tuple(config, current_messages, current_recall_memories)
        return {
            "answer": f"ì¸í„°ë„·ì—ì„œ '{search_query}' ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•œ ê²°ê³¼ì…ë‹ˆë‹¤:\n\n{results_text}",
            "evaluation_criteria": "í•´ë‹¹ ëª¨ë“œì—ì„œëŠ” í‰ê°€ ê¸°ì¤€ ì¶”ì¶œì´ ì œê³µë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
            "chat_no": chat_no
        }

    # ë¬¸ì„œ ì—†ìŒ + ì¼ë°˜ ì§ˆë¬¸
    else:
        # ì¼ë°˜ ëŒ€í™” ëª¨ë“œ
        agent_state = State(messages=current_messages, recall_memories=current_recall_memories)
        agent_state = memory_agent.load_memories(agent_state, config)
        recall_text = "\n".join(agent_state.recall_memories)
        system_prompt = prompt_extraction.make_general_question_prompt(question, recall_text)
        memory_agent.set_prompt(system_prompt)
        agent_response = memory_agent.agent(agent_state)

        all_messages = current_messages + [msg for msg in agent_response.messages if isinstance(msg, (AIMessage, HumanMessage))]
        chat_no = checkpoint.save_tuple(config, all_messages, agent_response.recall_memories)

        messages = get_from_state(agent_response, "messages", [])
        if not messages:
            raise RuntimeError("No messages found in final_state")
        
        agent_response_message = messages[-1]
        agent_response_content = getattr(agent_response_message, "content", str(agent_response_message))

        return {
            "answer": agent_response_content,
            "evaluation_criteria": "",
            "chat_no": chat_no
        }


def split_audio(file_path, chunk_length_ms=30000):
    audio = AudioSegment.from_file(file_path)
    chunks = []
    for i in range(0, len(audio), chunk_length_ms):
        end = i + chunk_length_ms
        if end > len(audio):
            end = len(audio)  
        chunk = audio[i:end]
        chunks.append(chunk)
    return chunks
async def transcribe_chunk(chunk_file, model):
    try:
        asr_result = model.transcribe(chunk_file)
        chunk_text = " ".join([
            seg["text"].strip()
            for seg in asr_result["segments"]
            if seg.get("language", "ko") == "ko"
        ])
        return chunk_text
    except Exception as e:
        print(f"Chunk transcribe error: {e}")
        return "[ì „ì‚¬ ì‹¤íŒ¨]"
    
@router.post("/transcribe_audio_chunked")
async def transcribe_audio_chunked(file: UploadFile = File(...)):
    # 1. ì„ì‹œ íŒŒì¼ ì €ì¥
    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
        tmp.write(await file.read())
        audio_path = tmp.name

    # 2. ëª¨ë¸ ë¡œë“œ (medium ëª¨ë¸)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    language = "ko"
    model = whisperx.load_model(
        "medium",
        device=device,
        language=language,
        compute_type="float32",  # ì •í™•ë„ ë†’ì„
        vad_method="silero"
    )

    # 3. ì˜¤ë””ì˜¤ ë¶„í•  (ëê¹Œì§€ ì •í™•íˆ)
    chunks = split_audio(audio_path, chunk_length_ms=30000)
    full_transcript = ""
    chunk_transcripts = []

    # 4. ê° chunk ì „ì‚¬ (ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”)
    for idx, chunk in enumerate(chunks):
        chunk_tmp_path = None
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as chunk_tmp:
                chunk.export(chunk_tmp.name, format="wav")
                chunk_tmp_path = chunk_tmp.name
            chunk_text = await transcribe_chunk(chunk_tmp_path, model)
            chunk_transcripts.append(chunk_text)
            full_transcript += chunk_text + " "
        except Exception as e:
            print(f"Chunk {idx} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            chunk_transcripts.append("[ì „ì‚¬ ì‹¤íŒ¨]")
            full_transcript += "[ì „ì‚¬ ì‹¤íŒ¨] "
        finally:
            if chunk_tmp_path and os.path.exists(chunk_tmp_path):
                os.remove(chunk_tmp_path)

    # 5. ì„ì‹œ íŒŒì¼ ì‚­ì œ
    if os.path.exists(audio_path):
        os.remove(audio_path)

    # 6. í›„ì²˜ë¦¬ (ollama ë“±ì€ ê·¸ëŒ€ë¡œ ìœ ì§€)
    return {
        "transcription": full_transcript.strip(),
        "chunk_transcripts": chunk_transcripts
    }


@router.post("/transcribe_audio")
async def transcribe_audio(file: UploadFile = File(...)):
    import whisperx
    import tempfile
    import os
    import ollama

    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
        tmp.write(await file.read())
        audio_path = tmp.name

    device = "cpu"
    language = "ko"
    model = whisperx.load_model("medium", device=device, language=language, compute_type="int8", vad_method="silero")

    asr_result = model.transcribe(audio_path)
    os.remove(audio_path)

    raw_transcript = " ".join([
        seg["text"].strip()
        for seg in asr_result["segments"]
        if seg.get("language", "ko") == "ko"
    ])

    prompt = prompt_extraction.make_light_cleaning_prompt(raw_transcript)
    response = ollama.generate(model="qwen2.5", prompt=prompt)
    lightly_cleaned = response["response"]

    return {"transcription": lightly_cleaned}

from pydantic import BaseModel

class TextRequest(BaseModel):
    text: str

@router.post("/summarize_text")
async def summarize_text(request: TextRequest):
    import ollama

    prompt = prompt_extraction.make_prompt(request.text)
    response = ollama.generate(model='qwen2.5', prompt=prompt)

    summary_raw = response["response"]
    summary_clean = clean_korean_only(summary_raw)

    return {"summary": summary_clean}

@router.post("/upload_audio")
async def upload_audio(file: UploadFile = File(...)):
    # 1. íŒŒì¼ ì €ì¥
    save_path = f"./call_data/{file.filename}"
    with open(save_path, "wb") as buffer:
        buffer.write(await file.read())

    # 2. WhisperX + LLM Q&A ì¶”ì¶œ
    qna_data = await process_audio_and_extract_qna(save_path)
    return JSONResponse(content={"qna": qna_data})

# ê¸°ì¡´ distinct_speaker_audio ì½”ë“œì—ì„œ ì‹¤ì œ Q&A ì¶”ì¶œ ë¶€ë¶„ë§Œ í•¨ìˆ˜ë¡œ ë¶„ë¦¬
import whisperx, json

async def process_audio_and_extract_qna(audio_path):
    device = "cpu"
    language = "ko"
    model = whisperx.load_model("medium", device=device, language=language, compute_type="int8", vad_method="silero")
    asr_result = model.transcribe(audio_path)

    transcript = " ".join([
        seg["text"].strip()
        for seg in asr_result["segments"]
        if seg.get("language", "ko") == "ko"
    ])
@router.post("/transcribe_audio_chunked")
async def transcribe_audio_chunked(file: UploadFile = File(...)):
    # 1. ì„ì‹œ íŒŒì¼ ì €ì¥
    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
        tmp.write(await file.read())
        audio_path = tmp.name

    # 2. ëª¨ë¸ ë¡œë“œ
    device = "cuda" if torch.cuda.is_available() else "cpu"
    language = "ko"
    try:
        model = whisperx.load_model(
            "large-v2",  # ë” í° ëª¨ë¸ë¡œ ë³€ê²½
            device=device,
            language=language,
            compute_type="float16",  # ì •í™•ë„ ë†’ì„
            vad_method="silero"
        )
    except Exception as e:
        print(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        model = whisperx.load_model(
            "medium",
            device=device,
            language=language,
            compute_type="int8",
            vad_method="silero"
        )

    # 3. ì˜¤ë””ì˜¤ ë¶„í• 
    chunks = split_audio(audio_path, chunk_length_ms=30000)
    full_transcript = ""
    chunk_transcripts = []

    # 4. ê° chunk ì „ì‚¬
    for idx, chunk in enumerate(chunks):
        chunk_tmp_path = None
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as chunk_tmp:
                chunk.export(chunk_tmp.name, format="wav")
                chunk_tmp_path = chunk_tmp.name
            chunk_text = await transcribe_chunk(chunk_tmp_path, model)
            chunk_transcripts.append(chunk_text)
            full_transcript += chunk_text + " "
        except Exception as e:
            print(f"Chunk {idx} ì „ì‚¬ ì‹¤íŒ¨: {e}")
            chunk_transcripts.append("[ì „ì‚¬ ì‹¤íŒ¨]")
            full_transcript += "[ì „ì‚¬ ì‹¤íŒ¨] "
        finally:
            if chunk_tmp_path and os.path.exists(chunk_tmp_path):
                os.remove(chunk_tmp_path)

    # 5. ì„ì‹œ íŒŒì¼ ì‚­ì œ
    if os.path.exists(audio_path):
        os.remove(audio_path)

    # 6. í›„ì²˜ë¦¬ (ì˜ˆ: ollamaë¡œ ì •ì œ)
    try:
        prompt = prompt_extraction.make_light_cleaning_prompt(full_transcript)
        response = ollama.generate(model="qwen2.5", prompt=prompt)
        lightly_cleaned = response["response"]
    except Exception as e:
        print(f"í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        lightly_cleaned = full_transcript

    return {
        "transcription": lightly_cleaned,
        "chunk_transcripts": chunk_transcripts
    }

@router.post("/transcribe_audio")
async def transcribe_audio(file: UploadFile = File(...)):
    import whisperx
    import tempfile
    import os
    import ollama

    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
        tmp.write(await file.read())
        audio_path = tmp.name

    device = "cpu"
    language = "ko"
    model = whisperx.load_model("medium", device=device, language=language, compute_type="int8", vad_method="silero")

    asr_result = model.transcribe(audio_path)
    os.remove(audio_path)

    raw_transcript = " ".join([
        seg["text"].strip()
        for seg in asr_result["segments"]
        if seg.get("language", "ko") == "ko"
    ])

    prompt = prompt_extraction.make_light_cleaning_prompt(raw_transcript)
    response = ollama.generate(model="qwen2.5", prompt=prompt)
    lightly_cleaned = response["response"]

    return {"transcription": lightly_cleaned}

from pydantic import BaseModel

class TextRequest(BaseModel):
    text: str

@router.post("/summarize_text")
async def summarize_text(request: TextRequest):
    import ollama

    prompt = prompt_extraction.make_prompt(request.text)
    response = ollama.generate(model='qwen2.5', prompt=prompt)

    summary_raw = response["response"]
    summary_clean = clean_korean_only(summary_raw)

    return {"summary": summary_clean}

@router.post("/upload_audio")
async def upload_audio(file: UploadFile = File(...)):
    # 1. íŒŒì¼ ì €ì¥
    save_path = f"./call_data/{file.filename}"
    with open(save_path, "wb") as buffer:
        buffer.write(await file.read())

    # 2. WhisperX + LLM Q&A ì¶”ì¶œ
    qna_data = await process_audio_and_extract_qna(save_path)
    return JSONResponse(content={"qna": qna_data})

# ê¸°ì¡´ distinct_speaker_audio ì½”ë“œì—ì„œ ì‹¤ì œ Q&A ì¶”ì¶œ ë¶€ë¶„ë§Œ í•¨ìˆ˜ë¡œ ë¶„ë¦¬
import whisperx, json

async def process_audio_and_extract_qna(audio_path):
    device = "cpu"
    language = "ko"
    model = whisperx.load_model("medium", device=device, language=language, compute_type="int8", vad_method="silero")
    asr_result = model.transcribe(audio_path)

    transcript = " ".join([
        seg["text"].strip()
        for seg in asr_result["segments"]
        if seg.get("language", "ko") == "ko"
    ])

    # 4. LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = prompt_extraction.make_audio_transcription_prompt(transcript)

    # Ollama Qwen2.5 ëª¨ë¸ë¡œ Q&A ë¶„ë¦¬ ìš”ì²­
    import ollama
    try:
        response = ollama.generate(
            model="qwen2.5",
            prompt=prompt
        )
        result_text = response['response'].strip()
        try:
            qna_data = json.loads(result_text)
        except json.JSONDecodeError:
            qna_data = []
            lines = result_text.splitlines()
            for i in range(0, len(lines), 2):
                if i+1 < len(lines):
                    question = lines[i].replace("ì§ˆë¬¸:", "").strip()
                    answer = lines[i+1].replace("ë‹µë³€:", "").strip()
                    qna_data.append({"question": question, "answer": answer})
        return qna_data
    except Exception as e:
        return [{"question": "Error", "answer": str(e)}]
# @router.post("/transcribe_audio")
# async def transcribe_audio(file: UploadFile = File(...)):
#     import whisperx
#     import tempfile
#     import os
#     import ollama

#     with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as tmp:
#         tmp.write(await file.read())
#         audio_path = tmp.name

#     device = "cpu"
#     language = "ko"
#     model = whisperx.load_model("medium", device=device, language=language, compute_type="int8", vad_method="silero")

#     asr_result = model.transcribe(audio_path)
#     os.remove(audio_path)

#     raw_transcript = " ".join([
#         seg["text"].strip()
#         for seg in asr_result["segments"]
#         if seg.get("language", "ko") == "ko"
#     ])

#     prompt = prompt_extraction.make_light_cleaning_prompt(raw_transcript)
#     response = ollama.generate(model="qwen2.5", prompt=prompt)
#     lightly_cleaned = response["response"]

#     return {"transcription": lightly_cleaned}

from pydantic import BaseModel

class TextRequest(BaseModel):
    text: str

@router.post("/summarize_text")
async def summarize_text(request: TextRequest):
    import ollama

    prompt = prompt_extraction.make_prompt(request.text)
    response = ollama.generate(model='qwen2.5', prompt=prompt)

    summary_raw = response["response"]
    summary_clean = clean_korean_only(summary_raw)

    return {"summary": summary_clean}


@router.post("/upload_audio")
async def upload_audio(file: UploadFile = File(...)):
    # 1. íŒŒì¼ ì €ì¥
    save_path = f"./call_data/{file.filename}"
    with open(save_path, "wb") as buffer:
        buffer.write(await file.read())

    # 2. WhisperX + LLM Q&A ì¶”ì¶œ
    # (ì•„ë˜ í•¨ìˆ˜ëŠ” ê¸°ì¡´ distinct_speaker_audio ì½”ë“œ í™œìš©)
    qna_data = await process_audio_and_extract_qna(save_path)
    return JSONResponse(content={"qna": qna_data})

# ê¸°ì¡´ distinct_speaker_audio ì½”ë“œì—ì„œ ì‹¤ì œ Q&A ì¶”ì¶œ ë¶€ë¶„ë§Œ í•¨ìˆ˜ë¡œ ë¶„ë¦¬
import whisperx, json

async def process_audio_and_extract_qna(audio_path):
    device = "cpu"
    language = "ko"
    model = whisperx.load_model("medium", device=device, language=language, compute_type="int8", vad_method="silero")
    asr_result = model.transcribe(audio_path)

    transcript = " ".join([
        seg["text"].strip()
        for seg in asr_result["segments"]
        if seg.get("language", "ko") == "ko"
    ])

    # 4. LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = prompt_extraction.make_audio_transcription_prompt(transcript)

    # Ollama Qwen2.5 ëª¨ë¸ë¡œ Q&A ë¶„ë¦¬ ìš”ì²­
    import ollama
    try:
        response = ollama.generate(
            model="qwen2.5",
            prompt=prompt
        )
        result_text = response['response'].strip()
        try:
            qna_data = json.loads(result_text)
        except json.JSONDecodeError:
            qna_data = []
            lines = result_text.splitlines()
            for i in range(0, len(lines), 2):
                if i+1 < len(lines):
                    question = lines[i].replace("ì§ˆë¬¸:", "").strip()
                    answer = lines[i+1].replace("ë‹µë³€:", "").strip()
                    qna_data.append({"question": question, "answer": answer})
        return qna_data
    except Exception as e:
        return [{"question": "Error", "answer": str(e)}]

# @router.post("/distinct_speaker_audio")
# async def distinct_speaker_audio():
#     # 1. ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
#     audio_path = "./call_data/05.mp3"

#     # 2. WhisperX ëª¨ë¸ ë¡œë“œ ë° ì „ì‚¬
#     device = "cpu"
#     language = "ko"
#     model = whisperx.load_model("medium", device=device, language=language, compute_type="int8", vad_method="silero")
#     asr_result = model.transcribe(audio_path)

#     # 3. í•œêµ­ì–´ ì „ì‚¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ
#     transcript = " ".join([
#         seg["text"].strip()
#         for seg in asr_result["segments"]
#         if seg.get("language", "ko") == "ko"
#     ])

#     # 4. LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
#     prompt = f"""
# ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•˜ì„¸ìš”.    
# ë‹¤ìŒ í…ìŠ¤íŠ¸ëŠ” ë¯¼ì› ì „í™” ìƒë‹´ ë‚´ìš©ì„ ì „ì‚¬í•œ ê²ƒì…ë‹ˆë‹¤. ì§ˆë¬¸ê³¼ ë‹µë³€ì„ êµ¬ë¶„í•´ JSON ë°°ì—´ í˜•íƒœë¡œ ë§Œë“¤ì–´ ì£¼ì„¸ìš”.
# ë°˜ë“œì‹œ JSON í˜•ì‹ë§Œ ì¶œë ¥í•´ ì£¼ì„¸ìš”.

# í˜•ì‹:
# [
#   {{ "question": "ì§ˆë¬¸ ë‚´ìš©", "answer": "ë‹µë³€ ë‚´ìš©" }},
#   ...
# ]

# í…ìŠ¤íŠ¸:
# \"\"\"{transcript}\"\"\"
# """

#     # 5. Ollama Qwen2.5 ëª¨ë¸ë¡œ Q&A ë¶„ë¦¬ ìš”ì²­
#     try:
#         response = ollama.generate(
#             model="qwen2.5",
#             prompt=prompt
#         )
#         result_text = response['response'].strip()

#         # 6. JSON íŒŒì‹± ì‹œë„
#         try:
#             qna_data = json.loads(result_text)
#         except json.JSONDecodeError:
#             # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ, fallback: ë‹¨ìˆœ í…ìŠ¤íŠ¸ íŒŒì‹± (ì§ˆë¬¸/ë‹µë³€ êµ¬ë¶„)
#             qna_data = []
#             lines = result_text.splitlines()
#             for i in range(0, len(lines), 2):
#                 if i+1 < len(lines):
#                     question = lines[i].replace("ì§ˆë¬¸:", "").strip()
#                     answer = lines[i+1].replace("ë‹µë³€:", "").strip()
#                     qna_data.append({"question": question, "answer": answer})

#         return JSONResponse(content={"qna": qna_data})

#     except Exception as e:
#         return JSONResponse(content={"error": str(e)}, status_code=500)


class QuestionInput(BaseModel):
    question: str
@router.post("/ask_query")
async def ask_query(input: QuestionInput):
    query = input.question
    async with httpx.AsyncClient(timeout=300.0) as client:
        search_resp = await client.post(
            "http://localhost:8002/api/search_vectors",
            json={"question": query, "collection_name": "wlmmate_vectors"}
        )

    raw_results = search_resp.json().get("result", [])
    if isinstance(raw_results, str):
        raw_results = [raw_results]

    context_text = "\n\n".join([r for r in raw_results if isinstance(r, str) and r.strip()])

    if not context_text or len(context_text) < 30:
        prompt = prompt_extraction.make_fallback_prompt(query)
    else:
        prompt = prompt_extraction.make_contextual_prompt(query, context_text)

    ollama = OllamaHosting(model="qwen2.5", prompt=prompt)
    final_answer = ollama.get_model_response().strip()

    return {"answer": final_answer}



@router.get("/api/chat_list")
async def chat_list(request: Request):
    employee = request.session.get("employee")
    emp_code = employee["emp_code"]

    checkpoint = MySQLCheckpoint(
        host=DB_HOST,
        port=DB_PORT,
        user=DB_USER,
        password=DB_PASSWORD,
        db=DB_NAME,
        charset=DB_CHARSET
    )
    return checkpoint.get_chat_list(emp_code)

@router.get("/api/chat_log")
async def chat_log(chat_no: int):
    checkpoint = MySQLCheckpoint(
        host=DB_HOST,
        port=DB_PORT,
        user=DB_USER,
        password=DB_PASSWORD,
        db=DB_NAME,
        charset=DB_CHARSET
    )
    return checkpoint.get_chat_log(chat_no)

@router.delete("/api/delete_chat_room")
async def delete_chat_room(chat_no: int, request: Request):
    employee = request.session.get("employee")
    emp_code = employee["emp_code"]

    checkpoint = MySQLCheckpoint(
        host=DB_HOST,
        port=DB_PORT,
        user=DB_USER,
        password=DB_PASSWORD,
        db=DB_NAME,
        charset=DB_CHARSET
    )

    if not checkpoint.is_chat_owner(chat_no, emp_code):
        raise HTTPException(status_code=403, detail="í•´ë‹¹ ì±„íŒ…ë°©ì— ëŒ€í•œ ì‚­ì œ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤.")

    conn = checkpoint._get_connection()
    try:
        with conn.cursor() as cursor:
            cursor.execute("DELETE FROM chat_log WHERE chat_no = %s", (chat_no,))
            cursor.execute("DELETE FROM chat_mate WHERE chat_no = %s", (chat_no,))
        conn.commit()
        return {"success": True}
    finally:
        conn.close()



### uvicorn main:app --reload
 