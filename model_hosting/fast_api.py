from fastapi import File, UploadFile, Form, APIRouter, Request, HTTPException
from fastapi.responses import JSONResponse
from typing import List, Optional, Literal, Any
import tempfile
import os
import re
import pymysql
from duckduckgo_search import DDGS  
from pydantic import BaseModel
from extraction.file_base_extraction import get_extractor_by_extension
# from extraction.pdf_extraction import PDFExtraction
from extraction.prompt_extraciont import PromptExtraction
from ollama_load.ollama_hosting import OllamaHosting
import requests
import httpx
from dotenv import load_dotenv

import whisperx
import ollama
import json
from langchain_core.messages import get_buffer_string, AIMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableConfig
from langchain_ollama import ChatOllama
from langgraph.graph import END, START, StateGraph
from langgraph.prebuilt import ToolNode
from transformers import AutoTokenizer
from langchain_huggingface.embeddings import HuggingFaceEmbeddings
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_core.documents import Document
from langchain_core.tools import tool, StructuredTool
import uuid


load_dotenv()
secret = os.getenv("SESSION_SECRET")

# í™˜ê²½ë³€ìˆ˜ì—ì„œ DB ì ‘ì† ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸° (ì—†ìœ¼ë©´ í•˜ë“œì½”ë”©)
DB_HOST = os.environ.get("MY_DB_HOST", "localhost")
DB_PORT = int(os.environ.get("MY_DB_PORT", "3306"))
DB_USER = os.environ.get("MY_DB_USER", "")
DB_PASSWORD = os.environ.get("MY_DB_PASSWORD", "")  # ì‹¤ì œ ë¹„ë°€ë²ˆí˜¸ë¡œ êµì²´
DB_NAME = os.environ.get("MY_DB_NAME", "wlb_mate")
DB_CHARSET = os.environ.get("MY_DB_CHARSET", "utf8mb4")

class State(BaseModel):
    messages: List[Any] = []
    recall_memories: List[str] = []

class EmbeddingManager:
    def __init__(self, model_name: str = "BM-K/KoSimCSE-roberta"):
        self.embeddings = HuggingFaceEmbeddings(
            model_name=model_name,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        self.vector_store = InMemoryVectorStore(
            embedding=self.embeddings
        )
    def get_embeddings(self):
        return self.embeddings
    def get_vector_store(self):
        return self.vector_store

class MemoryTools:
    def __init__(self, vector_store):
        self.vector_store = vector_store

        # 1. docstring ë°©ì‹
        self.save_recall_memory_tool = tool(self._save_recall_memory_with_doc)
        self.search_recall_memories_tool = tool(self._search_recall_memories_with_doc)

        # 2. description ì§ì ‘ ì§€ì • ë°©ì‹
        self.save_recall_memory_tool2 = StructuredTool.from_function(
            func=self._save_recall_memory_no_doc,
            name="save_recall_memory",
            description="Save user's long-term memory to vector storage."
        )
        self.search_recall_memories_tool2 = StructuredTool.from_function(
            func=self._search_recall_memories_no_doc,
            name="search_recall_memories",
            description="Search memories by semantic similarity."
        )

        # ë‘˜ ì¤‘ í•˜ë‚˜ë§Œ toolsë¡œ ì‚¬ìš©í•˜ì„¸ìš” (ì•„ë˜ëŠ” docstring ë°©ì‹)
        self.tools = [self.save_recall_memory_tool, self.search_recall_memories_tool]
        # self.tools = [self.save_recall_memory_tool2, self.search_recall_memories_tool2]

    def get_user_id(self, config: RunnableConfig) -> str:
        user_id = config["configurable"].get("user_id")
        if user_id is None:
            raise ValueError("User ID needs to be provided to save a memory.")
        return user_id

    # 1. docstring ë°©ì‹ (ì˜ì–´, Google-style)
    def _save_recall_memory_with_doc(self, memory: str, config: RunnableConfig) -> str:
        """
        Save user's long-term memory to the vector store.

        Args:
            memory (str): The memory text to save.
            config (RunnableConfig): Configuration containing user_id.

        Returns:
            str: The saved memory text.
        """
        user_id = self.get_user_id(config)
        document = Document(
            page_content=memory,
            id=str(uuid.uuid4()),
            metadata={"user_id": user_id}
        )
        self.vector_store.add_documents([document])
        return memory

    def _search_recall_memories_with_doc(self, query: str, config: RunnableConfig) -> list:
        """
        Search stored memories by semantic similarity.

        Args:
            query (str): The search query.
            config (RunnableConfig): Configuration containing user_id.

        Returns:
            list: List of matched memory texts.
        """
        user_id = self.get_user_id(config)
        def _filter_function(doc: Document) -> bool:
            return doc.metadata.get("user_id") == user_id
        documents = self.vector_store.similarity_search(
            query, k=3, filter=_filter_function
        )
        return [document.page_content for document in documents]

    # 2. description ë°©ì‹ (docstring ì—†ì´)
    def _save_recall_memory_no_doc(self, memory: str, config: RunnableConfig) -> str:
        user_id = self.get_user_id(config)
        document = Document(
            page_content=memory,
            id=str(uuid.uuid4()),
            metadata={"user_id": user_id}
        )
        self.vector_store.add_documents([document])
        return memory

    def _search_recall_memories_no_doc(self, query: str, config: RunnableConfig) -> list:
        user_id = self.get_user_id(config)
        def _filter_function(doc: Document) -> bool:
            return doc.metadata.get("user_id") == user_id
        documents = self.vector_store.similarity_search(
            query, k=3, filter=_filter_function
        )
        return [document.page_content for document in documents]

    def get_tools(self):
        return self.tools

class MySQLCheckpoint:
    """
    PyMySQL ì»¤ë„¥ì…˜ì„ ëª…ì‹œì ìœ¼ë¡œ ì—´ê³  ë‹«ëŠ” ì»¤ìŠ¤í…€ ì²´í¬í¬ì¸í„° ì˜ˆì‹œ.
    ê° ë©”ì„œë“œ í˜¸ì¶œ ì‹œ ìƒˆë¡œìš´ ì»¤ë„¥ì…˜ì„ ìƒì„±í•˜ê³  ë‹«ìŠµë‹ˆë‹¤.
    """
    def __init__(self, host, port, user, password, db, charset="utf8"):
        self.db_config = {
            "host": host,
            "port": port,
            "user": user,
            "password": password,
            "db": db,
            "charset": charset
        }
    
    def _get_connection(self):
        return pymysql.connect(**self.db_config)

    def get_tuple(self, config):
        emp_code = config["configurable"]["user_id"]
        conn = self._get_connection()
        try:
            with conn.cursor() as cursor:
                cursor.execute("""
                    SELECT log_text, log_speaker_sn
                    FROM chat_log
                    WHERE chat_no IN (
                        SELECT chat_no FROM chat_mate WHERE emp_no = (
                            SELECT emp_no FROM employee WHERE emp_code = %s
                        )
                    )
                    ORDER BY log_create_dt
                """, (emp_code,))
                rows = cursor.fetchall()

                messages = []
                for log_text, speaker_sn in rows:
                    if speaker_sn == 1:
                        messages.append(HumanMessage(content=log_text))
                    elif speaker_sn == 2:
                        messages.append(AIMessage(content=log_text))

                return {"messages": messages, "recall_memories": []}
        finally:
            conn.close()

    def save_tuple(self, config, messages: List[Any], recall_memories: List[str]):
        emp_code = config["configurable"]["user_id"]
        conn = self._get_connection()
        try:
            with conn.cursor() as cursor:
                cursor.execute("""
                    SELECT chat_no FROM chat_mate WHERE emp_no = (
                        SELECT emp_no FROM employee WHERE emp_code = %s
                    ) ORDER BY chat_create_dt DESC LIMIT 1
                """, (emp_code,))
                row = cursor.fetchone()
                chat_no = row[0] if row else None
                if not chat_no:
                    print("â— chat_no ì—†ìŒ - ìƒˆ chat_mate ìƒì„±")
                
                    # í˜„ì¬ í•´ë‹¹ emp_codeì˜ ëŒ€í™” ê°œìˆ˜ ì„¸ê¸°
                    cursor.execute("""
                        SELECT COUNT(*) FROM chat_mate 
                        WHERE emp_no = (SELECT emp_no FROM employee WHERE emp_code = %s)
                    """, (emp_code,))
                    count = cursor.fetchone()[0]
                
                    # ì œëª© ë§Œë“¤ê¸°: ëŒ€í™” 1, ëŒ€í™” 2, ...
                    title = f"ëŒ€í™” {count + 1}"
                
                    # chat_mateì— ìƒˆ row ì‚½ì…
                    cursor.execute("""
                        INSERT INTO chat_mate (emp_no, chat_title)
                        VALUES (
                            (SELECT emp_no FROM employee WHERE emp_code = %s),
                            %s
                        )
                    """, (emp_code, title))
                    conn.commit()
                
                    # ë°©ê¸ˆ ìƒì„±í•œ chat_no ê°€ì ¸ì˜¤ê¸°
                    cursor.execute("SELECT LAST_INSERT_ID()")
                    chat_no = cursor.fetchone()[0]

                for msg in messages:
                    speaker_sn = 1 if isinstance(msg, HumanMessage) else 2
                    cursor.execute("""
                        INSERT INTO chat_log (chat_no, log_text, log_speaker_sn, log_create_dt)
                        VALUES (%s, %s, %s, NOW())
                    """, (chat_no, msg.content, speaker_sn))
            conn.commit()
        finally:
            conn.close()


class MemoryAgent:
    def __init__(self, model_name: str = "qwen2.5", tokenizer_name: str = "BM-K/KoSimCSE-roberta"):
        self.model = ChatOllama(model=model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        self.prompt = self._create_prompt()
        self.tools = None 
        self.model_with_tools = None
    def _create_prompt(self) -> ChatPromptTemplate:
        return ChatPromptTemplate.from_messages([
            (
                "system",
                "ë‹¹ì‹ ì€ ê³ ê¸‰ ì¥ê¸° ê¸°ì–µë ¥ì„ ê°€ì§„ ìœ ìš©í•œ ì¡°ìˆ˜ì…ë‹ˆë‹¤."
                " ê¸°ëŠ¥. í•œêµ­ì–´ë¥¼ ì´í•´í•˜ëŠ” LLMìœ¼ë¡œ êµ¬ë™ë˜ë¯€ë¡œ ë‹µë³€ì€ í•œêµ­ì–´ë¡œ í•´ì•¼ í•©ë‹ˆë‹¤.."
                " ëŒ€í™” ì‚¬ì´ì˜ ì •ë³´ë¥¼ ì €ì¥í•˜ëŠ” ì™¸ì¥ ë©”ëª¨ë¦¬ê°€ ìˆìŠµë‹ˆë‹¤."
                " ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì €ì¥í•˜ê³  ê²€ìƒ‰í•˜ì„¸ìš”"
                " ì‚¬ìš©ìì˜ ì£¼ì˜ë¥¼ ê¸°ìš¸ì´ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” ì¤‘ìš”í•œ ì„¸ë¶€ ì‚¬í•­"
                "ë©”ëª¨ë¦¬ ì‚¬ìš© ì§€ì¹¨:\n"
                "1. ë©”ëª¨ë¦¬ ë„êµ¬(save_recall_memory)ë¥¼ ì ê·¹ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì„¸ìš”."
                " ì‚¬ìš©ìì— ëŒ€í•œ í¬ê´„ì ì¸ ì´í•´ë¥¼ êµ¬ì¶•í•˜ê¸° ìœ„í•´.\n"
                "2. ì €ì¥ëœ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •ë³´ì— ì…ê°í•œ ê°€ì •ê³¼ ì¶”ì •ì„ í•©ë‹ˆë‹¤."
                " ì¶”ì–µ.\n"
                "3. íŒ¨í„´ì„ ì‹ë³„í•˜ê¸° ìœ„í•´ ê³¼ê±° ìƒí˜¸ì‘ìš©ì„ ì •ê¸°ì ìœ¼ë¡œ ë°˜ì„±í•˜ê³ "
                " ê¸°ë³¸ ì„¤ì •.\n"
                "4. ìƒˆë¡œìš´ ì‘í’ˆë§ˆë‹¤ ì‚¬ìš©ìì˜ ì •ì‹  ëª¨ë¸ì„ ì—…ë°ì´íŠ¸í•˜ì„¸ìš”."
                " ì •ë³´.\n"
                "5. ìƒˆë¡œìš´ ì •ë³´ì™€ ê¸°ì¡´ ê¸°ì–µì„ ìƒí˜¸ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤."
                " ì¼ê´€ì„±.\n"
                "6. ê°ì •ì  ë§¥ë½ê³¼ ê°œì¸ì  ê°€ì¹˜ë¥¼ ì €ì¥í•˜ëŠ” ê²ƒì„ ìš°ì„ ì‹œí•©ë‹ˆë‹¤."
                " ì‚¬ì‹¤ê³¼ í•¨ê»˜.\n"
                "7. ê¸°ì–µì„ ì‚¬ìš©í•˜ì—¬ í•„ìš”ë¥¼ ì˜ˆì¸¡í•˜ê³  ì´ì— ëŒ€í•œ ì‘ë‹µì„ ë§ì¶¤í™”í•˜ì„¸ìš”."
                " ì‚¬ìš©ì ìŠ¤íƒ€ì¼.\n"
                "8. ì‚¬ìš©ìì˜ ìƒí™© ë³€í™”ë¥¼ ì¸ì‹í•˜ê³  ì¸ì •í•˜ê¸° ë˜ëŠ”"
                " ì‹œê°„ì— ë”°ë¥¸ ê´€ì .\n"
                "9. ê¸°ì–µì„ í™œìš©í•˜ì—¬ ê°œì¸í™”ëœ ì˜ˆì‹œë¥¼ ì œê³µí•©ë‹ˆë‹¤."
                " ìœ ì¶”.\n"
                "10. ê³¼ê±°ì˜ ë„ì „ì´ë‚˜ ì„±ê³µì„ íšŒìƒí•˜ì—¬ í˜„ì¬ë¥¼ ì•Œë¦½ë‹ˆë‹¤."
                " ë¬¸ì œ í•´ê²°.\n\n"
                "## ì¶”ì–µ íšŒìƒ\n"
                "ì†Œí™˜ ê¸°ì–µì€ í˜„ì¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë§¥ë½ì ìœ¼ë¡œ ê²€ìƒ‰ë©ë‹ˆë‹¤."
                " ëŒ€í™”:\n{recall_memories}\n\n"
                "## ì§€ì¹¨\n"
                "ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë™ë£Œë‚˜ ì¹œêµ¬ë¡œì„œ ìì—°ìŠ¤ëŸ½ê²Œ ì‚¬ìš©ìì™€ ì†Œí†µí•˜ì„¸ìš”."
                " ë‹¹ì‹ ì˜ ê¸°ì–µ ëŠ¥ë ¥ì„ ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰í•  í•„ìš”ëŠ” ì—†ìŠµë‹ˆë‹¤."
                " ëŒ€ì‹  ì‚¬ìš©ìì— ëŒ€í•œ ì´í•´ë¥¼ ì›í™œí•˜ê²Œ í†µí•©í•˜ì„¸ìš”."
                " ë‹¹ì‹ ì˜ ë°˜ì‘ì— ê·€ë¥¼ ê¸°ìš¸ì´ì„¸ìš”. ë¯¸ë¬˜í•œ ì‹ í˜¸ì™€ ê·¼ë³¸ì ì¸ ì •ë³´ì— ì£¼ì˜í•˜ì„¸ìš”."
                " ê°ì •. ì‚¬ìš©ìì˜ ì˜ì‚¬ì†Œí†µ ìŠ¤íƒ€ì¼ì— ë§ê²Œ ì¡°ì •í•˜ì„¸ìš”."
                " ì„ í˜¸ë„ì™€ í˜„ì¬ ê°ì • ìƒíƒœ. ì§€ì†í•˜ë ¤ë©´ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
                " ë‹¤ìŒ ëŒ€í™”ì—ì„œ ìœ ì§€í•˜ê³  ì‹¶ì€ ì •ë³´. ë§Œì•½ ë‹¹ì‹ ì´"
                " ë„êµ¬ í˜¸ì¶œì„ í•˜ì„¸ìš”. ë„êµ¬ í˜¸ì¶œ ì•ì— ìˆëŠ” ëª¨ë“  í…ìŠ¤íŠ¸ëŠ” ë‚´ë¶€ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤"
                " ë©”ì‹œì§€. ë„êµ¬ë¥¼ í˜¸ì¶œí•œ í›„ ì‘ë‹µí•˜ì„¸ìš”"
                " ë„êµ¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n\n",
            ),
            ("placeholder", "{messages}"),
        ])
    def setup_model_with_tools(self, tools):
        self.tools = tools
        self.model_with_tools = self.model.bind_tools(tools)
        return self.model_with_tools
    def agent(self, state: State) -> State:
        bound = self.prompt | self.model_with_tools
        recall_str = (
            "<recall_memory>\n" + "\n".join(state.recall_memories) + "\n</recall_memory>"
        )
        prediction = bound.invoke(
            {
                "messages": state.messages,
                "recall_memories": recall_str,
            }
        )
        return State(messages=[prediction], recall_memories=state.recall_memories)
    def load_memories(self, state: State, config: RunnableConfig) -> State:
        if not self.tools:
            raise ValueError("Tools have not been set up. Call setup_model_with_tools first.")
        convo_str = get_buffer_string(state.messages)
        tokens = self.tokenizer(convo_str, truncation=True, max_length=2048)
        truncated_text = self.tokenizer.decode(tokens['input_ids'])
        recall_memories = self.tools[1].invoke(truncated_text, config=config)
        return State(messages=state.messages, recall_memories=recall_memories)
    def route_tools(self, state: State) -> Literal["tools", END]:
        msg = state.messages[-1]
        if getattr(msg, "tool_calls", None):
            return "tools"
        return END

router = APIRouter()
prompt_extraction = PromptExtraction()

def search_web_duckduckgo(query: str):
    with DDGS() as ddgs:
        results = ddgs.text(query, region='kr-kr', max_results=3)
        return list(results)

def summarize_body(body: str, max_len=150) -> str:
    body = body.strip().replace("\n", " ")
    if len(body) > max_len:
        return body[:max_len].rstrip() + "..."
    return body

def clean_korean_only(text: str) -> str:
    return re.sub(r"[^ê°€-í£a-zA-Z0-9\s.,!?~\-]", "", text)

def classify_question_mode(question: str) -> str:
    keywords = ["ìœ ì‚¬ ì‚¬ì—…", "ì¸í„°ë„·ì—ì„œ ì°¾ì•„", "ì›¹ ê²€ìƒ‰", "ê²€ìƒ‰","ê²€ìƒ‰í•´ì¤˜"]
    if any(k in question.lower() for k in keywords):
        return "web_search"
    return "document"

embedding_manager = EmbeddingManager()
memory_tools = MemoryTools(embedding_manager.get_vector_store())
memory_agent = MemoryAgent()
memory_agent.setup_model_with_tools(memory_tools.get_tools())

def get_from_state(state, key, default):
    if hasattr(state, key):
        return getattr(state, key, default)
    elif isinstance(state, dict) and key in state:
        return state[key]
    elif hasattr(state, "values") and key in state.values:
        return state.values.get(key, default)
    return default

@router.post("/ask")
async def ask(
    request: Request,
    question: str = Form(...),
    files: List[UploadFile] = File(None)
):
    
    # ë””ë²„ê¹…ìš© ì„¸ì…˜ ë¡œê·¸ ì¶œë ¥
    print("ğŸ” request.session =", request.session)
    print("ğŸ” session.get('employee') =", request.session.get("employee"))
    print("ğŸ” request.cookies =", request.cookies)


    employee = request.session.get("employee")
    if not employee or "emp_code" not in employee:
        print("âŒ ì„¸ì…˜ ì¸ì¦ ì‹¤íŒ¨ - 401 ë°˜í™˜")
        raise HTTPException(status_code=401, detail="ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")

    user_id = employee["emp_code"]


    mode = classify_question_mode(question)
    config = {"configurable": {"user_id": user_id, "thread_id": user_id}}

    # MySQLCheckpoint ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìš”ì²­ë§ˆë‹¤ ìƒì„±í•˜ì—¬ ì—°ê²°ì„ ëª…ì‹œì ìœ¼ë¡œ ê´€ë¦¬
    checkpoint = MySQLCheckpoint(
        host=DB_HOST,
        port=DB_PORT,
        user=DB_USER,
        password=DB_PASSWORD,
        db=DB_NAME,
        charset=DB_CHARSET
    )
    
    current_state_from_checkpoint = checkpoint.get_tuple(config)
    current_messages = current_state_from_checkpoint.get("messages", [])
    current_recall_memories = current_state_from_checkpoint.get("recall_memories", [])

    # Ensure current_messages is a list and append the new HumanMessage
    current_messages = list(current_messages)
    current_messages.append(HumanMessage(content=question))

    if files:
        document_texts = []
        filenames = []

        for file in files[:5]:  # ìµœëŒ€ 5ê°œ ì²˜ë¦¬
            suffix = os.path.splitext(file.filename)[1]
            with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
                tmp.write(await file.read())
                tmp_path = tmp.name

            tmp.close()

            extractor = get_extractor_by_extension(file.filename, tmp_path)
            pages = extractor.extract_text()
            os.remove(tmp_path)

            text = "\n\n".join([p['text'] for p in pages])
            document_texts.append((file.filename, text))
            filenames.append(file.filename)

            page_texts = [p['text'] for p in pages]
            async with httpx.AsyncClient(timeout=300.0) as client:
                await client.post(
                    "http://localhost:8002/api/upload_vectors",
                    json={"chunks": page_texts, "collection_name": "qdrant_temp"}
                )


        async with httpx.AsyncClient(timeout=300.0) as client:
            search_resp = await client.post(
                "http://localhost:8002/api/search_vectors",
                json={"question": question, "collection_name": "qdrant_temp"}
            )

        search_data = search_resp.json()
        context_texts = search_data.get("result", "")
        context = "\n".join(context_texts if isinstance(context_texts, list) else [context_texts])


        # ì›¹ ê²€ìƒ‰ ëª¨ë“œ: ì²« ë²ˆì§¸ íŒŒì¼ ê¸°ì¤€ìœ¼ë¡œ ê²€ìƒ‰ì–´ ì¶”ì¶œ
        if mode == "web_search":
            first_text = document_texts[0][1]
            keyword_prompt = prompt_extraction.make_keyword_extraction_prompt(first_text)
            ollama_extract_keywords = OllamaHosting("qwen2.5", keyword_prompt)
            search_query = ollama_extract_keywords.get_model_response().strip()
            results = search_web_duckduckgo(search_query)

            results_text = "\n\n".join(
                [
                    f"ì œëª©: {res.get('title', '(ì œëª©ì—†ìŒ)')}\n"
                    f"ë‚´ìš©: {summarize_body(res.get('body', ''))}\n"
                    f"ì‚¬ì´íŠ¸ ì£¼ì†Œ: {res.get('href', '(ì£¼ì†Œì—†ìŒ)')}"
                    for res in results
                ]
            )
            # ì›¹ ê²€ìƒ‰ ê²°ê³¼ëŠ” DBì— ì €ì¥í•˜ì§€ ì•ŠëŠ” ê²ƒìœ¼ë¡œ íŒë‹¨í•˜ì—¬, ì´ì „ ë©”ì‹œì§€ë§Œ ì €ì¥
            checkpoint.save_tuple(config, current_messages, current_recall_memories)
            return {
                "answer": f"ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìœ ì‚¬ ì‚¬ì—…ì„ ê²€ìƒ‰í•œ ê²°ê³¼ì…ë‹ˆë‹¤ (ê²€ìƒ‰ì–´: {search_query}):\n\n{results_text}",
                "evaluation_criteria": "í•´ë‹¹ ëª¨ë“œì—ì„œëŠ” í‰ê°€ ê¸°ì¤€ ì¶”ì¶œì´ ì œê³µë˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
            }

        # ì¼ë°˜ ë¬¸ì„œ ì§ˆì˜ ì‘ë‹µ ëª¨ë“œ
        answers = []
        evaluation_criteria = None

        for filename, text in document_texts:
            # í•œ ë²ˆë§Œ ì¶”ì¶œ
            if evaluation_criteria is None:
                async with httpx.AsyncClient(timeout=300.0) as client:
                    criteria_resp = await client.post(
                        "http://localhost:8002/api/search_vectors",
                        json={"question": "í‰ê°€ ê¸°ì¤€", "collection_name": "qdrant_temp"}
                    )
                criteria_data = criteria_resp.json()
                criteria_list = criteria_data.get("result", "")
                evaluation_criteria = "\n".join(criteria_list if isinstance(criteria_list, list) else [criteria_list])

            qa_prompt = prompt_extraction.make_prompt_to_query_document(context, question)
            qa_ollama = OllamaHosting("qwen2.5", qa_prompt)
            answer = qa_ollama.get_model_response().strip()

            answers.append(f" **{filename}** ì—ì„œì˜ ì‘ë‹µ:\n{answer}")

        agent_response_content = "\n\n---\n\n".join(answers)
        checkpoint.save_tuple(config, current_messages, current_recall_memories)
        return {
            "answer": agent_response_content,
            "evaluation_criteria": evaluation_criteria
        }

    # ë¬¸ì„œ ì—†ìŒ + ì›¹ ê²€ìƒ‰
    elif mode == "web_search":
        search_query = question.strip()
        results = search_web_duckduckgo(search_query)
        results_text = "\n\n".join(
            [
                f"ì œëª©: {res.get('title','(ì œëª©ì—†ìŒ)')}\n"
                f"ë‚´ìš©: {summarize_body(res.get('body',''))}\n"
                f"ì‚¬ì´íŠ¸ ì£¼ì†Œ: {res.get('href','(ì£¼ì†Œì—†ìŒ)')}"
                for res in results
            ]
        )
        checkpoint.save_tuple(config, current_messages, current_recall_memories)
        return {
            "answer": f"ì¸í„°ë„·ì—ì„œ '{search_query}' ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•œ ê²°ê³¼ì…ë‹ˆë‹¤:\n\n{results_text}",
            "evaluation_criteria": "í•´ë‹¹ ëª¨ë“œì—ì„œëŠ” í‰ê°€ ê¸°ì¤€ ì¶”ì¶œì´ ì œê³µë˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
        }

    # ë¬¸ì„œ ì—†ìŒ + ì¼ë°˜ ì§ˆë¬¸
    else:
        # ì¼ë°˜ ëŒ€í™” ëª¨ë“œ
        # agent_state = State(messages=current_messages, recall_memories=current_recall_memories)
        agent_state = State(messages=current_messages, recall_memories=[])
        agent_state = memory_agent.load_memories(agent_state, config)
        agent_response = memory_agent.agent(agent_state)
        
        # agent_responseì—ì„œ AIMessage ê°ì²´ë§Œ ì¶”ì¶œí•˜ì—¬ ì €ì¥
        messages_to_save = [msg for msg in agent_response.messages if isinstance(msg, (AIMessage, HumanMessage))]
        
        checkpoint.save_tuple(config, messages_to_save, agent_response.recall_memories)
        
        messages = get_from_state(agent_response, "messages", [])
        if not messages:
            raise RuntimeError("No messages found in final_state")
        
        agent_response_message = messages[-1]
        agent_response_content = getattr(agent_response_message, "content", str(agent_response_message))

        return {
            "answer": agent_response_content,
            "evaluation_criteria": "ê¸°ì–µ ê¸°ë°˜ ì‘ë‹µ ëª¨ë“œì…ë‹ˆë‹¤. í‰ê°€ ê¸°ì¤€ì€ ì œê³µë˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
        }


@router.post("/transcribe_audio")
async def transcribe_audio(file: UploadFile = File(...)):
    import whisperx
    import tempfile
    import os
    import ollama

    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as tmp:
        tmp.write(await file.read())
        audio_path = tmp.name

    device = "cpu"
    language = "ko"
    model = whisperx.load_model("medium", device=device, language=language, compute_type="int8", vad_method="silero")

    asr_result = model.transcribe(audio_path)
    os.remove(audio_path)

    raw_transcript = " ".join([
        seg["text"].strip()
        for seg in asr_result["segments"]
        if seg.get("language", "ko") == "ko"
    ])

    prompt = prompt_extraction.make_light_cleaning_prompt(raw_transcript)
    response = ollama.generate(model="qwen2.5", prompt=prompt)
    lightly_cleaned = response["response"]

    return {"transcription": lightly_cleaned}

from pydantic import BaseModel

class TextRequest(BaseModel):
    text: str

@router.post("/summarize_text")
async def summarize_text(request: TextRequest):
    import ollama

    prompt = prompt_extraction.make_prompt(request.text)
    response = ollama.generate(model='qwen2.5', prompt=prompt)

    summary_raw = response["response"]
    summary_clean = clean_korean_only(summary_raw)

    return {"summary": summary_clean}


@router.post("/upload_audio")
async def upload_audio(file: UploadFile = File(...)):
    # 1. íŒŒì¼ ì €ì¥
    save_path = f"./call_data/{file.filename}"
    with open(save_path, "wb") as buffer:
        buffer.write(await file.read())

    # 2. WhisperX + LLM Q&A ì¶”ì¶œ
    # (ì•„ë˜ í•¨ìˆ˜ëŠ” ê¸°ì¡´ distinct_speaker_audio ì½”ë“œ í™œìš©)
    qna_data = await process_audio_and_extract_qna(save_path)
    return JSONResponse(content={"qna": qna_data})

# ê¸°ì¡´ distinct_speaker_audio ì½”ë“œì—ì„œ ì‹¤ì œ Q&A ì¶”ì¶œ ë¶€ë¶„ë§Œ í•¨ìˆ˜ë¡œ ë¶„ë¦¬
import whisperx, json

async def process_audio_and_extract_qna(audio_path):
    device = "cpu"
    language = "ko"
    model = whisperx.load_model("medium", device=device, language=language, compute_type="int8", vad_method="silero")
    asr_result = model.transcribe(audio_path)

    transcript = " ".join([
        seg["text"].strip()
        for seg in asr_result["segments"]
        if seg.get("language", "ko") == "ko"
    ])

    # 4. LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = prompt_extraction.make_audio_transcription_prompt(transcript)

    # Ollama Qwen2.5 ëª¨ë¸ë¡œ Q&A ë¶„ë¦¬ ìš”ì²­
    import ollama
    try:
        response = ollama.generate(
            model="qwen2.5",
            prompt=prompt
        )
        result_text = response['response'].strip()
        try:
            qna_data = json.loads(result_text)
        except json.JSONDecodeError:
            qna_data = []
            lines = result_text.splitlines()
            for i in range(0, len(lines), 2):
                if i+1 < len(lines):
                    question = lines[i].replace("ì§ˆë¬¸:", "").strip()
                    answer = lines[i+1].replace("ë‹µë³€:", "").strip()
                    qna_data.append({"question": question, "answer": answer})
        return qna_data
    except Exception as e:
        return [{"question": "Error", "answer": str(e)}]

# @router.post("/distinct_speaker_audio")
# async def distinct_speaker_audio():
#     # 1. ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
#     audio_path = "./call_data/05.mp3"

#     # 2. WhisperX ëª¨ë¸ ë¡œë“œ ë° ì „ì‚¬
#     device = "cpu"
#     language = "ko"
#     model = whisperx.load_model("medium", device=device, language=language, compute_type="int8", vad_method="silero")
#     asr_result = model.transcribe(audio_path)

#     # 3. í•œêµ­ì–´ ì „ì‚¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ
#     transcript = " ".join([
#         seg["text"].strip()
#         for seg in asr_result["segments"]
#         if seg.get("language", "ko") == "ko"
#     ])

#     # 4. LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
#     prompt = f"""
# ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•˜ì„¸ìš”.    
# ë‹¤ìŒ í…ìŠ¤íŠ¸ëŠ” ë¯¼ì› ì „í™” ìƒë‹´ ë‚´ìš©ì„ ì „ì‚¬í•œ ê²ƒì…ë‹ˆë‹¤. ì§ˆë¬¸ê³¼ ë‹µë³€ì„ êµ¬ë¶„í•´ JSON ë°°ì—´ í˜•íƒœë¡œ ë§Œë“¤ì–´ ì£¼ì„¸ìš”.
# ë°˜ë“œì‹œ JSON í˜•ì‹ë§Œ ì¶œë ¥í•´ ì£¼ì„¸ìš”.

# í˜•ì‹:
# [
#   {{ "question": "ì§ˆë¬¸ ë‚´ìš©", "answer": "ë‹µë³€ ë‚´ìš©" }},
#   ...
# ]

# í…ìŠ¤íŠ¸:
# \"\"\"{transcript}\"\"\"
# """

#     # 5. Ollama Qwen2.5 ëª¨ë¸ë¡œ Q&A ë¶„ë¦¬ ìš”ì²­
#     try:
#         response = ollama.generate(
#             model="qwen2.5",
#             prompt=prompt
#         )
#         result_text = response['response'].strip()

#         # 6. JSON íŒŒì‹± ì‹œë„
#         try:
#             qna_data = json.loads(result_text)
#         except json.JSONDecodeError:
#             # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ, fallback: ë‹¨ìˆœ í…ìŠ¤íŠ¸ íŒŒì‹± (ì§ˆë¬¸/ë‹µë³€ êµ¬ë¶„)
#             qna_data = []
#             lines = result_text.splitlines()
#             for i in range(0, len(lines), 2):
#                 if i+1 < len(lines):
#                     question = lines[i].replace("ì§ˆë¬¸:", "").strip()
#                     answer = lines[i+1].replace("ë‹µë³€:", "").strip()
#                     qna_data.append({"question": question, "answer": answer})

#         return JSONResponse(content={"qna": qna_data})

#     except Exception as e:
#         return JSONResponse(content={"error": str(e)}, status_code=500)


class QuestionInput(BaseModel):
    question: str
@router.post("/ask_query")
async def ask_query(input: QuestionInput):
    query = input.question
    async with httpx.AsyncClient(timeout=300.0) as client:
        search_resp = await client.post(
            "http://localhost:8002/api/search_vectors",
            json={"question": query, "collection_name": "wlmmate_vectors"}
        )

    raw_results = search_resp.json().get("result", [])
    if isinstance(raw_results, str):
        raw_results = [raw_results]

    context_text = "\n\n".join([r for r in raw_results if isinstance(r, str) and r.strip()])

    if not context_text or len(context_text) < 30:
        prompt = prompt_extraction.make_fallback_prompt(query)
    else:
        prompt = prompt_extraction.make_contextual_prompt(query, context_text)

    ollama = OllamaHosting(model="qwen2.5", prompt=prompt)
    final_answer = ollama.get_model_response().strip()

    return {"answer": final_answer}



### uvicorn main:app --reload
 