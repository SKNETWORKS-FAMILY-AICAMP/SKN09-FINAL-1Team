from fastapi import File, UploadFile, Form, APIRouter, Request, HTTPException, Query
from fastapi.responses import JSONResponse
from langchain_core.messages import AIMessage, HumanMessage
from typing import List
from dotenv import load_dotenv
import tempfile
import os
import torch
import whisperx
import ollama
from qdrant_db.qdrant_loader import init_qdrant_from_call_db, delete_point_by_id
from qdrant_db.qdrant_router import upload_vectors, search_vectors, delete_vectors, SearchRequest, UploadRequest
from model_hosting.extraction.file_base_extraction import get_extractor_by_extension
from model_hosting.extraction.prompt_extraction import PromptExtraction
from model_hosting.ollama_load.ollama_hosting import OllamaHosting
from model_hosting.module.module import feedback_model, State, TextRequest, QuestionInput, EmbeddingManager, MemoryTools, MySQLCheckpoint, MemoryAgent, search_web_duckduckgo, summarize_body, clean_korean_only, classify_question_mode, get_from_state, split_audio, transcribe_chunk, process_audio_and_extract_qna, init_qdrant_from_call_db
from pydantic import BaseModel
from fastapi.concurrency import run_in_threadpool
from pymysql.cursors import DictCursor



load_dotenv()
secret = os.getenv("SESSION_SECRET")
qdrant_url = os.getenv("QDRANT_URL")
embedding_manager = EmbeddingManager()
memory_tools = MemoryTools(embedding_manager.get_vector_store())
memory_agent = MemoryAgent()
memory_agent.setup_model_with_tools(memory_tools.get_tools())
prompt_extraction = PromptExtraction()

DB_HOST = os.environ.get("MY_DB_HOST", "localhost")
DB_PORT = int(os.environ.get("MY_DB_PORT", ""))
DB_USER = os.environ.get("MY_DB_USER", "")
DB_PASSWORD = os.environ.get("MY_DB_PASSWORD", "")  # ì‹¤ì œ ë¹„ë°€ë²ˆí˜¸ë¡œ êµì²´
DB_NAME = os.environ.get("MY_DB_NAME", "")
DB_CHARSET = os.environ.get("MY_DB_CHARSET", "utf8mb4")


router = APIRouter()


@router.post("/ask")
async def ask(
    request: Request,
    question: str = Form(...),
    files: List[UploadFile] = File(None)
):
    
    # ë””ë²„ê¹…ìš© ì„¸ì…˜ ë¡œê·¸ ì¶œë ¥
    print("ğŸ” request.session =", request.session)
    print("ğŸ” session.get('employee') =", request.session.get("employee"))
    print("ğŸ” request.cookies =", request.cookies)

    form = await request.form()
    new_chat_flag = form.get("new_chat", "false").lower() == "true"

    print("=>new_chat =", form.get("new_chat"))
    print("=>chat_no =", form.get("chat_no"))

    employee = request.session.get("employee")
    if not employee or "emp_code" not in employee:
        print("âŒ ì„¸ì…˜ ì¸ì¦ ì‹¤íŒ¨ - 401 ë°˜í™˜")
        raise HTTPException(status_code=401, detail="ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")

    user_id = employee["emp_code"]

    chat_no_str = form.get("chat_no", "").strip()
    chat_no = int(chat_no_str) if chat_no_str.isdigit() else None

    mode = classify_question_mode(question)
    config = {"configurable": {"user_id": user_id, "thread_id": user_id, "new_chat": new_chat_flag, "chat_no": chat_no}}

    # MySQLCheckpoint ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìš”ì²­ë§ˆë‹¤ ìƒì„±í•˜ì—¬ ì—°ê²°ì„ ëª…ì‹œì ìœ¼ë¡œ ê´€ë¦¬
    checkpoint = MySQLCheckpoint(
        host=DB_HOST,
        port=DB_PORT,
        user=DB_USER,
        password=DB_PASSWORD,
        db=DB_NAME,
        charset=DB_CHARSET
    )
    
    current_state_from_checkpoint = checkpoint.get_tuple(config)
    current_messages = current_state_from_checkpoint.get("messages", [])
    current_recall_memories = current_state_from_checkpoint.get("recall_memories", [])

    # Ensure current_messages is a list and append the new HumanMessage
    current_messages = list(current_messages)
    current_messages.append(HumanMessage(content=question))

    if files:
        document_texts = []
        filenames = []

        print("!! íŒŒì¼ ì…ë ¥ë¨")
        for file in files[:5]:  # ìµœëŒ€ 5ê°œ ì²˜ë¦¬
            suffix = os.path.splitext(file.filename)[1]
            with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
                tmp.write(await file.read())
                tmp_path = tmp.name

            tmp.close()

            extractor = get_extractor_by_extension(file.filename, tmp_path)
            pages = extractor.extract_text()
            os.remove(tmp_path)

            print("!! í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ")
            text = "\n\n".join([p['text'] for p in pages])
            document_texts.append((file.filename, text))
            filenames.append(file.filename)

            page_texts = [p['text'] for p in pages]

            upload_vectors(UploadRequest(chunks=page_texts, collection_name="qdrant_temp"))


        context_texts = search_vectors(SearchRequest(question=question, collection_name="qdrant_temp"))
        print(context_texts)
        context = "\n".join(context_texts if isinstance(context_texts, list) else [context_texts])
        print(context)

        # ì›¹ ê²€ìƒ‰ ëª¨ë“œ: ì²« ë²ˆì§¸ íŒŒì¼ ê¸°ì¤€ìœ¼ë¡œ ê²€ìƒ‰ì–´ ì¶”ì¶œ
        if mode == "web_search":
            first_text = document_texts[0][1]
            keyword_prompt = prompt_extraction.make_keyword_extraction_prompt(first_text)
            ollama_extract_keywords = OllamaHosting("qwen2.5", keyword_prompt)
            search_query = ollama_extract_keywords.get_model_response().strip()
            results = search_web_duckduckgo(search_query)

            results_text = "\n\n".join(
                [
                    f"ì œëª©: {res.get('title', '(ì œëª©ì—†ìŒ)')}\n"
                    f"ë‚´ìš©: {summarize_body(res.get('body', ''))}\n"
                    f"ì‚¬ì´íŠ¸ ì£¼ì†Œ: {res.get('href', '(ì£¼ì†Œì—†ìŒ)')}"
                    for res in results
                ]
            )
            # ì›¹ ê²€ìƒ‰ ê²°ê³¼ëŠ” DBì— ì €ì¥í•˜ì§€ ì•ŠëŠ” ê²ƒìœ¼ë¡œ íŒë‹¨í•˜ì—¬, ì´ì „ ë©”ì‹œì§€ë§Œ ì €ì¥
            chat_no = checkpoint.save_tuple(config, current_messages, current_recall_memories)
            return {
                "answer": f"ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìœ ì‚¬ ì‚¬ì—…ì„ ê²€ìƒ‰í•œ ê²°ê³¼ì…ë‹ˆë‹¤ (ê²€ìƒ‰ì–´: {search_query}):\n\n{results_text}",
                "evaluation_criteria": "í•´ë‹¹ ëª¨ë“œì—ì„œëŠ” í‰ê°€ ê¸°ì¤€ ì¶”ì¶œì´ ì œê³µë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
                "chat_no": chat_no
            }

        # ì¼ë°˜ ë¬¸ì„œ ì§ˆì˜ ì‘ë‹µ ëª¨ë“œ
        answers = []
        evaluation_criteria = None

        for filename, text in document_texts:
            # í•œ ë²ˆë§Œ ì¶”ì¶œ
            if evaluation_criteria is None:
                criteria_list = search_vectors(SearchRequest(question="í‰ê°€ ê¸°ì¤€", collection_name="qdrant_temp"))
                evaluation_criteria = "\n".join(criteria_list if isinstance(criteria_list, list) else [criteria_list])

            agent_state = State(messages=current_messages, recall_memories=current_recall_memories)
            agent_state = memory_agent.load_memories(agent_state, config)
            recall_memories_text = "\n".join(agent_state.recall_memories)

            qa_prompt = prompt_extraction.make_prompt_to_query_document(context, question, recall_memories_text)
            memory_agent.set_prompt(qa_prompt)

            agent_response = memory_agent.agent(agent_state)
            answer = get_from_state(agent_response, "messages", [])[-1].content.strip()

            answers.append(f"{answer}\n\n**ì¶œì²˜: {filename}**")

        agent_response_content = "\n\n---\n\n".join(answers)
        ai_message = AIMessage(content=agent_response_content)
        all_messages = current_messages + [ai_message]
        chat_no = checkpoint.save_tuple(config, all_messages, current_recall_memories)
        return {
            "answer": agent_response_content,
            "evaluation_criteria": evaluation_criteria,
            "chat_no": chat_no
        }

    # ë¬¸ì„œ ì—†ìŒ + ì›¹ ê²€ìƒ‰
    elif mode == "web_search":
        search_query = question.strip()
        results = search_web_duckduckgo(search_query)
        results_text = "\n\n".join(
            [
                f"ì œëª©: {res.get('title','(ì œëª©ì—†ìŒ)')}\n"
                f"ë‚´ìš©: {summarize_body(res.get('body',''))}\n"
                f"ì‚¬ì´íŠ¸ ì£¼ì†Œ: {res.get('href','(ì£¼ì†Œì—†ìŒ)')}"
                for res in results
            ]
        )
        chat_no = checkpoint.save_tuple(config, current_messages, current_recall_memories)
        return {
            "answer": f"ì¸í„°ë„·ì—ì„œ '{search_query}' ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•œ ê²°ê³¼ì…ë‹ˆë‹¤:\n\n{results_text}",
            "evaluation_criteria": "í•´ë‹¹ ëª¨ë“œì—ì„œëŠ” í‰ê°€ ê¸°ì¤€ ì¶”ì¶œì´ ì œê³µë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
            "chat_no": chat_no
        }

    # ë¬¸ì„œ ì—†ìŒ + ì¼ë°˜ ì§ˆë¬¸
    else:
        # ì¼ë°˜ ëŒ€í™” ëª¨ë“œ
        agent_state = State(messages=current_messages, recall_memories=current_recall_memories)
        agent_state = memory_agent.load_memories(agent_state, config)
        recall_text = "\n".join(agent_state.recall_memories)
        system_prompt = prompt_extraction.make_general_question_prompt(question, recall_text)
        memory_agent.set_prompt(system_prompt)
        agent_response = memory_agent.agent(agent_state)

        all_messages = current_messages + [msg for msg in agent_response.messages if isinstance(msg, (AIMessage, HumanMessage))]
        chat_no = checkpoint.save_tuple(config, all_messages, agent_response.recall_memories)

        messages = get_from_state(agent_response, "messages", [])
        if not messages:
            raise RuntimeError("No messages found in final_state")
        
        agent_response_message = messages[-1]
        agent_response_content = getattr(agent_response_message, "content", str(agent_response_message))

        return {
            "answer": agent_response_content,
            "evaluation_criteria": "",
            "chat_no": chat_no
        }


@router.post("/miniask")
async def miniask(input: QuestionInput):
    question = input.question.strip()

    # ë²¡í„° ê²€ìƒ‰
    raw_results = search_vectors(SearchRequest(question=question, collection_name="wlmmate_all"))

    # raw_results = search_resp.json().get("result", [])
    # if isinstance(raw_results, str):
    #     raw_results = [raw_results]

    context_text = "\n\n".join([r for r in raw_results if isinstance(r, str) and r.strip()])

    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    if not context_text or len(context_text) < 30:
        prompt = prompt_extraction.make_fallback_prompt(question)
    else:
        prompt = prompt_extraction.make_contextual_prompt(question, context_text)

    # ëª¨ë¸ í˜¸ì¶œ
    ollama_instance = OllamaHosting(model="qwen2.5", prompt=prompt)
    answer = ollama_instance.get_model_response().strip()

    return {"answer": answer}
    
    
@router.post("/transcribe_audio_chunked")
async def transcribe_audio_chunked(file: UploadFile = File(...)):
    # 1. ì„ì‹œ íŒŒì¼ ì €ì¥
    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
        tmp.write(await file.read())
        audio_path = tmp.name

    # 2. ëª¨ë¸ ë¡œë“œ (medium ëª¨ë¸)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    language = "ko"
    model = whisperx.load_model(
        "medium",
        device=device,
        language=language,
        compute_type="float32",  # ì •í™•ë„ ë†’ì„
        vad_method="silero"
    )

    # 3. ì˜¤ë””ì˜¤ ë¶„í•  (ëê¹Œì§€ ì •í™•íˆ)
    chunks = split_audio(audio_path, chunk_length_ms=30000)
    full_transcript = ""
    chunk_transcripts = []

    # 4. ê° chunk ì „ì‚¬ (ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”)
    for idx, chunk in enumerate(chunks):
        chunk_tmp_path = None
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as chunk_tmp:
                chunk.export(chunk_tmp.name, format="wav")
                chunk_tmp_path = chunk_tmp.name
            chunk_text = await transcribe_chunk(chunk_tmp_path, model)
            chunk_transcripts.append(chunk_text)
            full_transcript += chunk_text + " "
        except Exception as e:
            print(f"Chunk {idx} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            chunk_transcripts.append("[ì „ì‚¬ ì‹¤íŒ¨]")
            full_transcript += "[ì „ì‚¬ ì‹¤íŒ¨] "
        finally:
            if chunk_tmp_path and os.path.exists(chunk_tmp_path):
                os.remove(chunk_tmp_path)

    # 5. ì„ì‹œ íŒŒì¼ ì‚­ì œ
    if os.path.exists(audio_path):
        os.remove(audio_path)

    # 6. í›„ì²˜ë¦¬ (ollama ë“±ì€ ê·¸ëŒ€ë¡œ ìœ ì§€)
    return {
        "transcription": full_transcript.strip(),
        "chunk_transcripts": chunk_transcripts
    }


@router.post("/transcribe_audio")
async def transcribe_audio(file: UploadFile = File(...)):
    import whisperx
    import tempfile
    import os
    import ollama

    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
        tmp.write(await file.read())
        audio_path = tmp.name

    device = "cpu"
    language = "ko"
    model = whisperx.load_model("medium", device=device, language=language, compute_type="int8", vad_method="silero")

    asr_result = model.transcribe(audio_path)
    os.remove(audio_path)

    raw_transcript = " ".join([
        seg["text"].strip()
        for seg in asr_result["segments"]
        if seg.get("language", "ko") == "ko"
    ])

    prompt = prompt_extraction.make_light_cleaning_prompt(raw_transcript)
    response = ollama.generate(model="qwen2.5", prompt=prompt)
    lightly_cleaned = response["response"]

    return {"transcription": lightly_cleaned}



@router.post("/summarize_text")
async def summarize_text(request: TextRequest):
    import ollama

    prompt = prompt_extraction.make_prompt(request.text)
    response = ollama.generate(model='qwen2.5', prompt=prompt)

    summary_raw = response["response"]
    summary_clean = clean_korean_only(summary_raw)

    return {"summary": summary_clean}

@router.post("/upload_audio")
async def upload_audio(file: UploadFile = File(...)):        
        # 1. íŒŒì¼ ì €ì¥
    save_path = f"./call_data/{file.filename}"
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    with open(save_path, "wb") as buffer:
        buffer.write(await file.read())

    # 2. WhisperX + LLM Q&A ì¶”ì¶œ (ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°˜í™˜)
    qna_data = await process_audio_and_extract_qna(save_path)
    
    # 3. í”¼ë“œë°± ëª¨ë¸ í˜¸ì¶œ ë° ê° QnAì— í”¼ë“œë°± ì¶”ê°€
    feedbacks = feedback_model(qna_data)
    for i, qna in enumerate(qna_data):
        qna['feedback'] = feedbacks[i] if i < len(feedbacks) else ""
    
    init_qdrant_from_call_db()

    return JSONResponse(content={"qna": qna_data})

COLLECTIONS = [
    "wlmmate_civil",
    "wlmmate_directive",
    "wlmmate_law"
]

async def classify_collection_with_llm(question: str) -> str:
    prompt = f"""
    ë‹¤ìŒ ì§ˆë¬¸ì€ ì–´ë–¤ ì£¼ì œì— ê°€ì¥ ì í•©í•œê°€ìš”?
    - ë¯¼ì‚¬, ê°œì¸, ìƒí™œ, ë¯¼ì› (wlmmate_civil)
    - ì§€ì‹œ, ê·œì •, ì •ì±…, í–‰ì • (wlmmate_directive)
    - ë²•ë¥ , ë²•ë ¹, ê·œì •, íŒë¡€ (wlmmate_law)

    ì§ˆë¬¸: {question}
    ë‹µë³€ í˜•ì‹: "ì»¬ë ‰ì…˜ëª…"
    """
    try:
        result = await run_in_threadpool(
            lambda: OllamaHosting(model="qwen2.5", prompt=prompt).get_model_response().strip()
        )
        if result not in COLLECTIONS:
            return "wlmmate_civil"
        return result
    except Exception as e:
        print(f"ë¶„ë¥˜ ì‹¤íŒ¨: {e}, ê¸°ë³¸ê°’ civil ì‚¬ìš©")
        return "wlmmate_civil"

def search_appropriate_collection_sync(question: str, collection: str) -> list[str]:
    try:
        results = search_vectors(SearchRequest(question=question, collection_name=collection))
        if results and len(results) > 0:
            first_result = results[0]
            if isinstance(first_result, str):
                return [first_result]
            elif isinstance(first_result, (list, tuple)) and len(first_result) > 0:
                return [first_result[0]]
        return []
    except Exception as e:
        print(f"ê²€ìƒ‰ ì‹¤íŒ¨ ({collection}): {e}")
        return []

async def search_appropriate_collection(question: str) -> list[str]:
    collection = await classify_collection_with_llm(question)
    print(f"ë¶„ë¥˜ëœ ì»¬ë ‰ì…˜: {collection}")
    return await run_in_threadpool(lambda: search_appropriate_collection_sync(question, collection))

@router.post("/ask_query")
async def ask_query(input: QuestionInput):
    question = input.question.strip()
    print(f"ì§ˆë¬¸ ì²˜ë¦¬: {question}")

    contexts = await search_appropriate_collection(question)
    context_text = "\n\n".join(contexts)
    print(f"ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(context_text)}")

    if not context_text or len(context_text) < 30:
        prompt = prompt_extraction.make_fallback_prompt(question)
        print("fallback í”„ë¡¬í”„íŠ¸ ì‚¬ìš©")
    else:
        prompt = prompt_extraction.make_contextual_prompt(question, context_text)
        print("ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©")

    final_answer = await run_in_threadpool(
        lambda: OllamaHosting(model="qwen2.5", prompt=prompt).get_model_response().strip()
    )
    print(f"ìƒì„±ëœ ë‹µë³€: {final_answer[:100]}...")
    return {"answer": final_answer}


@router.post("/generate-unanswered")
async def generate_unanswered():
    checkpoint = MySQLCheckpoint(
        host=DB_HOST,
        port=DB_PORT,
        user=DB_USER,
        password=DB_PASSWORD,
        db=DB_NAME,
        charset=DB_CHARSET
    )
    conn = checkpoint._get_connection()
    try:
        with conn.cursor(DictCursor) as cursor:
            cursor.execute("""
                SELECT query_mate.query_no, query_mate.query_text
                FROM query_mate
                LEFT JOIN query_response ON query_mate.query_no = query_response.query_no
                WHERE query_response.res_text IS NULL OR query_response.res_text = '' OR query_response.res_text = 'null'
            """)
            unanswered = cursor.fetchall()

        for q in unanswered:
            try:
                input_data = QuestionInput(question=q["query_text"])
                result = await ask_query(input_data)
                answer = result.get("answer", "").strip()
                default_emp_no = 1

                with conn.cursor(DictCursor) as cursor:
                    cursor.execute("""
                        INSERT INTO query_response (query_no, emp_no, res_text, res_state, res_write_dt)
                        VALUES (%s, %s, %s, %s, NOW())
                        ON DUPLICATE KEY UPDATE
                            res_text = VALUES(res_text),
                            res_write_dt = NOW()
                    """, (q["query_no"], default_emp_no, answer, 1))
                conn.commit()

            except Exception as e:
                print(f"query_no={q['query_no']} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue

        return {"success": True, "count": len(unanswered)}

    except Exception as e:
        print(f"ì „ì²´ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return {"success": False}
    finally:
        conn.close()

@router.get("/chat_list")
async def chat_list(request: Request):
    employee = request.session.get("employee")
    if not employee:
        raise HTTPException(status_code=401, detail="ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    
    emp_code = employee["emp_code"]

    checkpoint = MySQLCheckpoint(
        host=DB_HOST,
        port=DB_PORT,
        user=DB_USER,
        password=DB_PASSWORD,
        db=DB_NAME,
        charset=DB_CHARSET
    )
    return checkpoint.get_chat_list(emp_code)


@router.get("/chat_log")
async def chat_log(chat_no: int):
    checkpoint = MySQLCheckpoint(
        host=DB_HOST,
        port=DB_PORT,
        user=DB_USER,
        password=DB_PASSWORD,
        db=DB_NAME,
        charset=DB_CHARSET
    )
    return checkpoint.get_chat_log(chat_no)

@router.delete("/delete_chat_room")
async def delete_chat_room(chat_no: int, request: Request):
    employee = request.session.get("employee")
    emp_code = employee["emp_code"]

    checkpoint = MySQLCheckpoint(
        host=DB_HOST,
        port=DB_PORT,
        user=DB_USER,
        password=DB_PASSWORD,
        db=DB_NAME,
        charset=DB_CHARSET
    )

    if not checkpoint.is_chat_owner(chat_no, emp_code):
        raise HTTPException(status_code=403, detail="í•´ë‹¹ ì±„íŒ…ë°©ì— ëŒ€í•œ ì‚­ì œ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤.")

    conn = checkpoint._get_connection()
    try:
        with conn.cursor() as cursor:
            cursor.execute("DELETE FROM chat_log WHERE chat_no = %s", (chat_no,))
            cursor.execute("DELETE FROM chat_mate WHERE chat_no = %s", (chat_no,))
        conn.commit()
        return {"success": True}
    finally:
        conn.close()

@router.post("/create_vectors")
def upload_vectors(collection_name="wlmmate_call"):
    init_qdrant_from_call_db(collection_name=collection_name)
    return {"success": True, "deleted_collection": collection_name}

@router.delete("/delete_vectors")
def delete_conn_vectors(collection_name: str = Query(..., description="ì‚­ì œí•  Qdrant ì»¬ë ‰ì…˜ ì´ë¦„")):
    delete_vectors(collection_name)
    print(f"ì»¬ë ‰ì…˜ ì‚­ì œ: {collection_name}")
    print(f"!! {collection_name} ì»¬ë ‰ì…˜ ì‚­ì œ")
    return {"success": True, "deleted_collection": collection_name}
    

@router.delete("/delete_vector_by_id")
def delete_vector_by_id(
    collection_name: str = Query(...),
    point_id: int = Query(...)
):
    success = delete_point_by_id(collection_name, point_id)
    if success:
        return {"message": f"Point {point_id} deleted from {collection_name}"}
    else:
        return {"error": "Collection not found or deletion failed"}


@router.get("/check-session")
async def check_session(request: Request):
    if "employee" not in request.session:
        raise HTTPException(status_code=401, detail="ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    
    return {"employee": request.session["employee"]}



### uvicorn main:app --reload
 